[2025-04-13 14:01:05,641] INFO 87.208.3.218 - - [13/Apr/2025:14:01:05 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2167 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:01:09,094] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:02:36,895] INFO 87.208.3.218 - - [13/Apr/2025:14:02:36 +0000] "GET /connectors HTTP/1.1" 200 48 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:04:09,098] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:08:32,612] INFO [AdminClient clientId=connect-cluster-shared-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 14:09:31,323] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:09:31,323] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:09:31,324] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:09:31,324] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:09:31,324] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:09:31,325] INFO 87.208.3.218 - - [13/Apr/2025:14:09:31 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 13 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:09:31,326] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:09:31,327] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:09:31,327] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:09:31,329] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=38, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:09:31,332] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=38, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:09:31,332] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:09:31,332] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:09:31,332] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:09:31,332] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:09:31,333] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:09:31,334] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:09:31,334] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 38 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=49, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:09:31,335] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 49 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:09:31,335] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:09:31,335] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:09:31,335] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:09:31,339] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=39, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:09:31,342] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=39, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:09:31,342] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 39 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=49, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:09:31,342] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 49 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:09:31,343] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:14:55,434] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:14:55,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:14:55,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:14:55,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:14:55,441] INFO 87.208.3.218 - - [13/Apr/2025:14:14:55 +0000] "POST /connectors HTTP/1.1" 201 702 "-" "PostmanRuntime/7.43.3" 14 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:14:55,444] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=40, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:14:55,446] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=40, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:14:55,447] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 40 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=50, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:14:55,447] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 50 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:14:55,447] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:14:55,447] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:14:55,447] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:14:55,447] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:14:55,448] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:14:55,448] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:14:55,448] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:14:55,449] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:14:55,449] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:14:55,449] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:14:55,459] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:14:55,460] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:14:55,461] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:14:55,462] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=41, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:14:55,463] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=41, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:14:55,463] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 41 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=52, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:14:55,463] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 52 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:14:55,463] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:14:55,463] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:14:55,464] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:14:55,464] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:14:55,464] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:14:55,464] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:14:55,464] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:14:55,465] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:14:55,465] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:14:55,465] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the connector config (org.apache.kafka.connect.runtime.Worker:685)
[2025-04-13 14:14:55,465] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:14:55,465] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:14:55,465] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:14:55,466] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:14:55,466] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:14:55,466] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:14:55,469] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:14:55,469] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:14:55,469] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:14:55,469] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744553695469 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:14:55,469] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:14:55,470] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:14:55,470] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:14:55,470] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;databaseName=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product>
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:14:55,471] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:14:55,471] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:14:55,471] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:14:55,476] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:14:55,476] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:14:55,478] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:14:55,480] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-4ccef187-9ec8-4e66-b3a3-6f9d8451699b (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:14:55,480] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:14:58,482] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-4ccef187-9ec8-4e66-b3a3-6f9d8451699b', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:14:58,483] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-0-4ccef187-9ec8-4e66-b3a3-6f9d8451699b=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:14:58,484] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-4ccef187-9ec8-4e66-b3a3-6f9d8451699b', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:14:58,484] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:14:58,484] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:14:58,485] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:14:58,489] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:14:58,510] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:14:58,573] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:14:58,605] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresValue$2(RecordValidator.java:86)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:81)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:14:58,605] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresValue$2(RecordValidator.java:86)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:81)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
[2025-04-13 14:14:58,605] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:14:58,605] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:14:58,606] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:14:58,606] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-4ccef187-9ec8-4e66-b3a3-6f9d8451699b sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:14:58,606] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:14:58,606] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:14:58,608] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:14:58,608] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:14:58,608] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:14:58,608] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:14:58,610] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:15:02,484] INFO 87.208.3.218 - - [13/Apr/2025:14:15:02 +0000] "GET /connectors HTTP/1.1" 200 48 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:15:05,874] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2483)
[2025-04-13 14:15:08,620] INFO 87.208.3.218 - - [13/Apr/2025:14:15:08 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2158 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:16:12,239] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:16:12,239] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:16:12,241] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:16:12,241] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:16:12,241] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:16:12,242] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:16:12,243] INFO 87.208.3.218 - - [13/Apr/2025:14:16:12 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 8 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:16:12,243] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:16:12,243] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:16:12,245] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=42, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:16:12,247] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=42, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:16:12,247] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:16:12,247] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:16:12,247] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:16:12,247] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:16:12,248] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:16:12,249] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:16:12,249] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 42 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=55, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:16:12,250] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 55 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:16:12,250] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:16:12,250] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:16:12,250] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:16:12,253] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=43, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:16:12,255] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=43, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:16:12,255] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 43 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=55, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:16:12,255] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 55 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:16:12,255] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:16:44,397] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:16:44,401] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:16:44,401] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:16:44,402] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:16:44,402] INFO 87.208.3.218 - - [13/Apr/2025:14:16:44 +0000] "POST /connectors HTTP/1.1" 201 702 "-" "PostmanRuntime/7.43.3" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:16:44,403] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=44, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:16:44,405] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=44, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:16:44,405] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 44 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=56, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:16:44,405] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 56 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:16:44,405] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:16:44,405] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:16:44,406] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:16:44,406] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:16:44,407] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:16:44,407] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:16:44,407] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:16:44,408] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:16:44,408] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:16:44,408] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:16:44,415] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:16:44,416] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:16:44,416] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:16:44,417] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=45, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:16:44,419] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=45, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:16:44,419] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 45 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=58, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:16:44,420] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 58 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:16:44,420] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:16:44,420] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:16:44,420] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the connector config (org.apache.kafka.connect.runtime.Worker:685)
[2025-04-13 14:16:44,421] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:16:44,422] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:16:44,422] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:16:44,422] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:16:44,422] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:16:44,423] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:16:44,425] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:16:44,425] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:16:44,425] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:16:44,425] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744553804425 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:16:44,426] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:16:44,426] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:16:44,426] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;databaseName=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product>
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:16:44,427] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:16:44,427] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:16:44,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:16:44,428] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:16:44,431] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:16:44,433] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:16:44,434] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:16:44,439] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-ae74d523-9a0d-4948-83ae-ec24932c422a (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:16:44,439] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:16:47,444] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-ae74d523-9a0d-4948-83ae-ec24932c422a', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:16:47,444] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 3: {connector-consumer-sink-to-database02-0-ae74d523-9a0d-4948-83ae-ec24932c422a=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:16:47,445] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-ae74d523-9a0d-4948-83ae-ec24932c422a', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:16:47,446] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:16:47,446] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:16:47,446] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:16:47,447] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:16:47,476] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:16:47,539] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:16:47,587] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresValue$2(RecordValidator.java:86)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:81)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:16:47,587] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresValue$2(RecordValidator.java:86)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:81)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
[2025-04-13 14:16:47,587] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:16:47,587] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:16:47,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:16:47,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-ae74d523-9a0d-4948-83ae-ec24932c422a sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:16:47,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:16:47,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:16:47,592] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:16:47,592] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:16:47,592] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:16:47,592] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:16:47,593] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:23:33,008] INFO [AdminClient clientId=connect-cluster-shared-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 14:24:37,703] INFO 87.208.3.218 - - [13/Apr/2025:14:24:37 +0000] "GET /connectors HTTP/1.1" 200 48 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:24:43,381] INFO 87.208.3.218 - - [13/Apr/2025:14:24:43 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2158 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:24:55,347] INFO 87.208.3.218 - - [13/Apr/2025:14:24:55 +0000] "GET /connectors HTTP/1.1" 200 48 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:00,190] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:25:00,190] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:25:00,192] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:25:00,192] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:25:00,192] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:25:00,192] INFO 87.208.3.218 - - [13/Apr/2025:14:25:00 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:00,193] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:25:00,193] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:00,193] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:00,195] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=46, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:00,197] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=46, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:00,197] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:25:00,197] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:25:00,197] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:25:00,197] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:25:00,198] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:25:00,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:25:00,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 46 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=60, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:00,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 60 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:00,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:00,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:00,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:00,202] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=47, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:00,203] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=47, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:00,204] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 47 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=60, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:00,204] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 60 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:00,204] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:04,577] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:25:04,583] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:25:04,583] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:04,583] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:04,585] INFO 87.208.3.218 - - [13/Apr/2025:14:25:04 +0000] "POST /connectors HTTP/1.1" 201 702 "-" "PostmanRuntime/7.43.3" 11 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:04,586] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=48, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:04,588] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=48, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:04,588] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 48 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=61, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:04,588] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 61 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:04,589] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:25:04,589] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:25:04,589] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:25:04,589] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:04,589] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:25:04,590] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:25:04,590] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:04,591] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:25:04,591] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:04,591] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:25:04,599] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:25:04,601] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:04,601] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:04,602] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=49, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:04,604] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=49, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:04,605] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 49 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=63, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:04,605] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 63 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:04,605] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:25:04,605] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:25:04,606] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the connector config (org.apache.kafka.connect.runtime.Worker:685)
[2025-04-13 14:25:04,607] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:25:04,607] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:25:04,607] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:25:04,607] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:04,608] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:25:04,608] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:25:04,610] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:25:04,610] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:25:04,610] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:25:04,610] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744554304610 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:25:04,611] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:25:04,611] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:25:04,611] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;databaseName=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product>
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:25:04,611] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:25:04,611] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:25:04,611] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:25:04,612] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:04,615] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:25:04,618] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:25:04,619] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:25:04,620] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-c4a50a25-030d-4208-a729-482f7643107c (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:25:04,621] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:25:07,624] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-c4a50a25-030d-4208-a729-482f7643107c', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:25:07,624] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-0-c4a50a25-030d-4208-a729-482f7643107c=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:25:07,625] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-c4a50a25-030d-4208-a729-482f7643107c', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:25:07,626] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:25:07,626] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:25:07,626] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:25:07,629] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:25:07,648] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:25:07,697] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:25:07,733] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema. (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresValue$2(RecordValidator.java:86)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:81)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:25:07,734] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: Sink connector 'sink-to-database02' is configured with 'delete.enabled=false' and 'pk.mode=record_value' and therefore requires records with a non-null Struct value and non-null Struct schema, but found record at (topic='azure-sql-.database01.SalesLT.Product',partition=0,offset=0,timestamp=1744550654448) with a HashMap value and null value schema.
	at io.confluent.connect.jdbc.sink.RecordValidator.lambda$requiresValue$2(RecordValidator.java:86)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:81)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
[2025-04-13 14:25:07,734] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:25:07,734] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:25:07,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:25:07,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-c4a50a25-030d-4208-a729-482f7643107c sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:25:07,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:25:07,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:25:07,737] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:25:07,737] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:25:07,737] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:25:07,737] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:25:07,739] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:25:09,988] INFO 87.208.3.218 - - [13/Apr/2025:14:25:09 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2158 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:17,175] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:25:17,175] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:25:17,175] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:25:17,175] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:25:17,175] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:25:17,175] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:25:17,176] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:17,176] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:17,178] INFO 87.208.3.218 - - [13/Apr/2025:14:25:17 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 10 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:17,180] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=50, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:17,182] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=50, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:17,182] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:25:17,182] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:25:17,182] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:25:17,182] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:25:17,183] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:25:17,184] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:25:17,185] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 50 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=65, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:17,185] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 65 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:17,185] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:17,185] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:17,185] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:17,187] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=51, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:17,190] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=51, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:17,190] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 51 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=65, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:17,190] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 65 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:17,190] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:26,736] INFO 87.208.3.218 - - [13/Apr/2025:14:25:26 +0000] "GET /connectors HTTP/1.1" 200 27 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:45,636] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:64)
com.fasterxml.jackson.databind.JsonMappingException: Unexpected character ('}' (code 125)): was expecting double-quote to start field name
 at [Source: REDACTED (`StreamReadFeature.INCLUDE_SOURCE_IN_LOCATION` disabled); line: 17, column: 4] (through reference chain: org.apache.kafka.connect.runtime.rest.entities.CreateConnectorRequest["config"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1937)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:572)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:440)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1493)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:348)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:185)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:342)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:2099)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1249)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:801)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:919)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:290)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:478)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:400)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:256)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:191)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('}' (code 125)): was expecting double-quote to start field name
 at [Source: REDACTED (`StreamReadFeature.INCLUDE_SOURCE_IN_LOCATION` disabled); line: 17, column: 4]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2481)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:752)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:676)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:2147)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1797)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1084)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:608)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:449)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:32)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)
	... 71 more
[2025-04-13 14:25:45,637] INFO 87.208.3.218 - - [13/Apr/2025:14:25:45 +0000] "POST /connectors HTTP/1.1" 500 328 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:49,697] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:25:49,700] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:25:49,701] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:49,702] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:49,702] INFO 87.208.3.218 - - [13/Apr/2025:14:25:49 +0000] "POST /connectors HTTP/1.1" 201 597 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:49,703] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=52, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:49,705] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=52, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:49,705] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 52 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=66, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:49,705] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 66 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:49,705] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:25:49,705] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:25:49,706] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:25:49,706] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:49,707] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:25:49,707] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:25:49,707] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:49,707] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:25:49,707] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:49,708] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:25:49,715] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:25:49,715] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:25:49,715] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:25:49,717] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=53, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:25:49,718] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=53, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:25:49,719] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 53 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=68, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:25:49,719] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 68 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:25:49,719] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:25:49,719] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:25:49,720] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:25:49,720] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:49,720] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:25:49,720] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:25:49,721] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:25:49,722] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:25:49,722] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:25:49,724] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:25:49,724] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:25:49,724] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:25:49,724] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744554349724 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:25:49,725] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:25:49,725] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:25:49,725] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:25:49,725] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;databaseName=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product>
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:25:49,725] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:25:49,725] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:25:49,726] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:25:49,729] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:25:49,730] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:25:49,730] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:25:49,733] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-a50469ac-fe90-4ecb-95ff-b76467356a7e (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:25:49,733] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:25:52,734] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-a50469ac-fe90-4ecb-95ff-b76467356a7e', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:25:52,734] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 3: {connector-consumer-sink-to-database02-0-a50469ac-fe90-4ecb-95ff-b76467356a7e=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:25:52,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-a50469ac-fe90-4ecb-95ff-b76467356a7e', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:25:52,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:25:52,735] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:25:52,736] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:25:52,739] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:25:52,756] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:25:52,806] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:25:52,840] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:25:52,842] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:25:52,842] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:25:52,842] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:25:52,843] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:25:54,663] INFO 87.208.3.218 - - [13/Apr/2025:14:25:54 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:25:55,843] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:25:55,886] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:25:55,933] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:25:55,934] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:25:55,935] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:25:55,935] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:25:55,935] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:25:58,936] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:25:58,995] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:25:59,044] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:25:59,046] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:25:59,046] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:25:59,046] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:25:59,046] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:02,047] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:02,084] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:02,127] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:02,129] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:02,130] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:02,130] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:02,130] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:05,130] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:05,173] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:05,233] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:05,237] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:05,237] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:05,237] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:05,238] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:08,239] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:08,277] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:08,326] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:08,327] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:08,328] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:08,328] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:08,328] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:11,329] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:11,372] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:11,408] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:11,410] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:11,410] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:11,411] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:11,411] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:14,412] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:14,448] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:14,502] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:14,504] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:14,504] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:14,504] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:14,504] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:17,505] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:17,551] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:17,591] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:17,592] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:17,592] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:17,593] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:17,593] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:18,704] INFO 87.208.3.218 - - [13/Apr/2025:14:26:18 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:26:20,373] INFO [azure-sql-server-source|task-0] 3 records sent during previous 00:34:15.415, last recorded offset of {server=azure-sql-, database=database01} partition is {transaction_id=null, event_serial_no=1, commit_lsn=00000111:00000b90:000c, change_lsn=00000111:00000b90:0004} (io.debezium.connector.common.BaseSourceTask:213)
[2025-04-13 14:26:20,594] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:20,629] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:20,666] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:20,668] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:20,668] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:20,669] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:26:20,669] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:21,527] INFO 87.208.3.218 - - [13/Apr/2025:14:26:21 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:26:22,883] INFO 87.208.3.218 - - [13/Apr/2025:14:26:22 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:26:23,669] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:26:23,726] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:26:23,764] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product>" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:26:23,765] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:26:23,766] ERROR [sink-to-database02|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 14:26:23,766] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:23,766] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:26:23,766] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:26:23,766] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:26:23,767] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:26:23,767] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] The pause flag in partitions [azure-sql-.database01.SalesLT.Product-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 14:26:23,767] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-a50469ac-fe90-4ecb-95ff-b76467356a7e sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:26:23,767] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:26:23,767] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:26:23,769] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:26:23,770] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:26:23,770] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:26:23,770] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:26:23,772] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:26:24,268] INFO 87.208.3.218 - - [13/Apr/2025:14:26:24 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2167 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:27:09,107] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:31:51,829] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:31:51,829] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:31:51,829] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:31:51,829] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:31:51,829] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:31:51,830] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:31:51,830] INFO 87.208.3.218 - - [13/Apr/2025:14:31:51 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:31:51,831] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:31:51,831] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:31:51,833] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=54, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:31:51,834] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=54, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:31:51,834] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:31:51,834] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:31:51,834] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:31:51,834] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:31:51,835] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:31:51,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:31:51,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 54 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=70, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:31:51,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 70 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:31:51,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:31:51,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:31:51,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:31:51,838] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=55, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:31:51,841] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=55, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:31:51,841] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 55 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=70, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:31:51,841] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 70 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:31:51,841] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:32:29,227] INFO 87.208.3.218 - - [13/Apr/2025:14:32:29 +0000] "GET /connectors HTTP/1.1" 200 27 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:33:23,892] INFO 87.208.3.218 - - [13/Apr/2025:14:33:23 +0000] "GET /connectors HTTP/1.1" 200 27 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:33:28,535] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:33:28,539] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:33:28,539] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:33:28,539] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:33:28,540] INFO 87.208.3.218 - - [13/Apr/2025:14:33:28 +0000] "POST /connectors HTTP/1.1" 201 592 "-" "PostmanRuntime/7.43.3" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:33:28,540] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=56, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:33:28,542] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=56, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:33:28,542] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 56 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=71, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:33:28,542] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 71 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:33:28,542] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:33:28,542] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:33:28,543] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:33:28,543] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:33:28,543] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:33:28,543] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:33:28,543] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:33:28,544] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:33:28,544] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:33:28,544] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:33:28,553] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:33:28,554] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:33:28,554] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:33:28,556] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=57, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:33:28,557] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=57, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:33:28,557] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 57 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=73, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:33:28,557] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 73 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:33:28,557] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:33:28,558] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:33:28,559] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:33:28,560] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:33:28,562] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:33:28,562] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:33:28,562] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:33:28,562] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744554808562 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:33:28,563] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:33:28,563] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:33:28,563] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:33:28,563] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:33:28,563] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:28,563] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:33:28,563] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:33:28,567] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:33:28,568] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:33:28,569] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:33:28,571] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-8cf3b7bb-c9aa-4f16-adfa-9ebe523ac137 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:33:28,571] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:33:31,572] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-8cf3b7bb-c9aa-4f16-adfa-9ebe523ac137', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:33:31,572] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-0-8cf3b7bb-c9aa-4f16-adfa-9ebe523ac137=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:33:31,574] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-8cf3b7bb-c9aa-4f16-adfa-9ebe523ac137', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:33:31,574] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:33:31,574] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:33:31,575] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:33:31,576] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:33:31,625] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:31,720] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:31,766] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:31,768] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:31,769] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:31,769] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:31,769] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:33,274] INFO 87.208.3.218 - - [13/Apr/2025:14:33:33 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:33:34,770] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:34,804] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:34,838] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:34,841] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:34,841] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:34,842] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:34,842] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:37,843] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:37,891] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:37,926] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:37,928] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:37,928] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:37,928] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:37,928] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:40,929] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:40,967] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:41,008] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:41,010] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:41,010] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:41,010] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:41,010] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:44,011] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:44,049] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:44,082] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:44,083] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:44,084] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:44,084] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:44,084] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:47,085] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:47,133] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:47,176] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:47,178] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:47,178] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:47,178] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:47,178] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:50,178] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:50,220] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:50,252] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:50,256] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:50,256] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:50,257] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:50,257] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:53,258] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:53,296] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:53,335] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:53,339] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:53,339] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:53,339] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:53,339] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:56,340] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:56,378] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:56,415] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:56,417] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:56,417] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:56,417] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:56,418] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:33:59,418] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:33:59,455] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:33:59,496] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:33:59,498] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:33:59,498] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:33:59,499] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:33:59,499] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:34:02,499] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:34:02,553] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:34:02,585] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:34:02,586] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:34:02,586] ERROR [sink-to-database02|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 14:34:02,586] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:34:02,587] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:34:02,587] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:34:02,587] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:34:02,587] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:34:02,587] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] The pause flag in partitions [azure-sql-.database01.SalesLT.Product-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 14:34:02,587] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-8cf3b7bb-c9aa-4f16-adfa-9ebe523ac137 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:34:02,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:34:02,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:34:02,590] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:34:02,590] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:34:02,590] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:34:02,590] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:34:02,593] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:34:07,948] INFO 87.208.3.218 - - [13/Apr/2025:14:34:07 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2167 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:34:09,111] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:35:59,363] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:35:59,363] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:35:59,364] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:35:59,364] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:35:59,364] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:35:59,364] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:35:59,364] INFO 87.208.3.218 - - [13/Apr/2025:14:35:59 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:35:59,364] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:35:59,364] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:35:59,366] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=58, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:35:59,369] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=58, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:35:59,370] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:35:59,370] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:35:59,370] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:35:59,370] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:35:59,370] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:35:59,371] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:35:59,371] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 58 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=75, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:35:59,373] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 75 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:35:59,373] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:35:59,373] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:35:59,373] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:35:59,375] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=59, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:35:59,376] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=59, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:35:59,377] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 59 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=75, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:35:59,377] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 75 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:35:59,377] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:37:13,713] INFO 87.208.3.218 - - [13/Apr/2025:14:37:13 +0000] "GET /connectors HTTP/1.1" 200 27 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:19,089] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:37:19,093] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:37:19,094] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:37:19,094] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:37:19,095] INFO 87.208.3.218 - - [13/Apr/2025:14:37:19 +0000] "POST /connectors HTTP/1.1" 201 801 "-" "PostmanRuntime/7.43.3" 10 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:19,095] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=60, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:37:19,098] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=60, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:37:19,098] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 60 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=76, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:37:19,098] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 76 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:37:19,098] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:37:19,098] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:37:19,098] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:37:19,099] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:37:19,099] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:37:19,099] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:37:19,099] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:37:19,100] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:37:19,100] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:37:19,100] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:37:19,110] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:37:19,112] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:37:19,112] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:37:19,113] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=61, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:37:19,114] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=61, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:37:19,114] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 61 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=78, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:37:19,114] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 78 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:37:19,115] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:37:19,115] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:37:19,115] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [dropPrefix]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:37:19,115] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [dropPrefix]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:37:19,115] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:37:19,115] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:37:19,116] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:37:19,116] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:37:19,116] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:37:19,116] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:37:19,116] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:37:19,118] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.RegexRouter} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:37:19,118] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:37:19,118] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:37:19,118] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:37:19,119] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:37:19,123] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:37:19,123] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:37:19,123] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:37:19,123] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744555039123 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:37:19,124] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:37:19,124] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:37:19,124] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:37:19,124] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:37:19,124] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:19,124] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:37:19,124] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:37:19,128] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:37:19,129] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:37:19,130] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:37:19,132] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-0b24588d-2b2e-4276-b7a8-f50144913ae1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:37:19,132] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:37:22,133] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-0b24588d-2b2e-4276-b7a8-f50144913ae1', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:37:22,134] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 3: {connector-consumer-sink-to-database02-0-0b24588d-2b2e-4276-b7a8-f50144913ae1=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:37:22,136] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-0b24588d-2b2e-4276-b7a8-f50144913ae1', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:37:22,137] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:37:22,137] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:37:22,138] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:37:22,139] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:37:22,165] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:22,220] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:22,252] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:22,254] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:22,254] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:22,254] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:22,254] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:23,478] INFO 87.208.3.218 - - [13/Apr/2025:14:37:23 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:25,256] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:25,291] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:25,330] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:25,331] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:25,331] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:25,331] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:25,331] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:28,333] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:28,371] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:28,409] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:28,422] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:28,422] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:28,422] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:28,422] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:31,422] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:31,458] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:31,491] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:31,492] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:31,493] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:31,493] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:31,493] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:34,494] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:34,531] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:34,564] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:34,567] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:34,567] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:34,567] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:34,567] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:37,567] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:37,602] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:37,640] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:37,642] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:37,643] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:37,643] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:37,643] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:40,643] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:40,704] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:40,739] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:40,741] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:40,741] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:40,741] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:40,741] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:43,741] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:43,785] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:43,817] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:43,818] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:43,819] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:43,819] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:43,819] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:46,819] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:46,857] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:46,890] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:46,893] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:46,893] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:46,893] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:46,893] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:47,380] INFO 87.208.3.218 - - [13/Apr/2025:14:37:47 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:49,894] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:49,933] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:49,939] INFO 87.208.3.218 - - [13/Apr/2025:14:37:49 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:49,975] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:49,977] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:49,977] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:49,977] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:37:49,977] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:51,822] INFO 87.208.3.218 - - [13/Apr/2025:14:37:51 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:52,978] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:37:53,022] INFO 87.208.3.218 - - [13/Apr/2025:14:37:53 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:37:53,055] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:37:53,088] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:37:53,090] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:37:53,090] ERROR [sink-to-database02|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 14:37:53,090] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:53,090] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:37:53,090] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:37:53,090] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:37:53,091] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:37:53,091] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] The pause flag in partitions [azure-sql-.database01.SalesLT.Product-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 14:37:53,091] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-0b24588d-2b2e-4276-b7a8-f50144913ae1 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:37:53,091] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:37:53,091] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:37:53,093] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:37:53,093] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:37:53,093] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:37:53,093] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:37:53,094] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:38:06,433] INFO 87.208.3.218 - - [13/Apr/2025:14:38:06 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2167 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:38:09,115] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:41:37,999] INFO 87.208.3.218 - - [13/Apr/2025:14:41:37 +0000] "GET /connectors HTTP/1.1" 200 48 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:41:40,996] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:41:40,996] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:41:40,998] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:41:40,998] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:41:40,998] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:41:40,998] INFO 87.208.3.218 - - [13/Apr/2025:14:41:40 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:41:40,999] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:41:40,999] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:41:40,999] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:41:41,001] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=62, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:41:41,002] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=62, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:41:41,003] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:41:41,003] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:41:41,003] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:41:41,003] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:41:41,004] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:41:41,005] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:41:41,005] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 62 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=80, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:41:41,005] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 80 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:41:41,005] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:41:41,005] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:41:41,005] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:41:41,007] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=63, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:41:41,009] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=63, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:41:41,009] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 63 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=80, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:41:41,009] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 80 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:41:41,009] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:41:45,803] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:41:45,807] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:41:45,807] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:41:45,807] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:41:45,808] INFO 87.208.3.218 - - [13/Apr/2025:14:41:45 +0000] "POST /connectors HTTP/1.1" 201 591 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:41:45,809] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=64, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:41:45,811] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=64, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:41:45,812] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 64 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=81, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:41:45,812] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 81 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:41:45,812] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:41:45,812] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:41:45,812] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:41:45,812] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:41:45,812] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:41:45,813] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:41:45,813] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:41:45,813] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:41:45,813] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:41:45,813] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:41:45,819] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:41:45,820] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:41:45,821] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:41:45,823] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=65, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:41:45,825] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=65, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:41:45,825] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 65 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=83, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:41:45,825] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 83 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:41:45,825] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:41:45,825] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:41:45,826] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:41:45,826] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:41:45,826] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:41:45,826] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:41:45,826] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:41:45,827] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:41:45,828] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:41:45,828] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:41:45,830] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:41:45,830] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:41:45,830] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:41:45,830] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744555305830 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:41:45,831] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:41:45,831] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure_sql_database01_SalesLT_Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:41:45,831] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:41:45,831] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:41:45,831] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:41:45,831] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:41:45,832] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:41:45,837] WARN [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Error while fetching metadata with correlation id 2 : {azure_sql_database01_SalesLT_Product=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 14:41:45,837] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:41:45,837] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:41:45,838] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:41:45,841] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-5d71bde4-301c-4a99-904b-f47c0b8df4d5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:41:45,842] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:41:48,844] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-5d71bde4-301c-4a99-904b-f47c0b8df4d5', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:41:48,845] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-0-5d71bde4-301c-4a99-904b-f47c0b8df4d5=Assignment(partitions=[azure_sql_database01_SalesLT_Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:41:48,846] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-5d71bde4-301c-4a99-904b-f47c0b8df4d5', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:41:48,846] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure_sql_database01_SalesLT_Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:41:48,846] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure_sql_database01_SalesLT_Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:41:48,847] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure_sql_database01_SalesLT_Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:41:48,847] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure_sql_database01_SalesLT_Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:41:51,318] INFO 87.208.3.218 - - [13/Apr/2025:14:41:51 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:24,378] INFO 87.208.3.218 - - [13/Apr/2025:14:42:24 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:27,759] INFO 87.208.3.218 - - [13/Apr/2025:14:42:27 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:31,050] INFO 87.208.3.218 - - [13/Apr/2025:14:42:31 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:34,105] INFO 87.208.3.218 - - [13/Apr/2025:14:42:34 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:45,410] INFO 87.208.3.218 - - [13/Apr/2025:14:42:45 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:48,048] INFO 87.208.3.218 - - [13/Apr/2025:14:42:48 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:42:58,511] INFO 87.208.3.218 - - [13/Apr/2025:14:42:58 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:43:09,119] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:43:11,733] INFO 87.208.3.218 - - [13/Apr/2025:14:43:11 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:43:15,395] INFO 87.208.3.218 - - [13/Apr/2025:14:43:15 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 1 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:44:26,659] INFO 87.208.3.218 - - [13/Apr/2025:14:44:26 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:44:49,861] INFO 127.0.0.1 - - [13/Apr/2025:14:44:49 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "curl/7.81.0" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:45:49,378] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:45:49,378] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:45:49,378] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:45:49,378] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:45:49,378] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:45:49,379] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:45:49,379] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:45:49,379] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:45:49,379] INFO 87.208.3.218 - - [13/Apr/2025:14:45:49 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:45:49,381] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=66, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:45:49,383] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=66, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:45:49,383] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:45:49,383] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:45:49,383] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:45:49,383] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:45:49,383] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:45:49,384] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure_sql_database01_SalesLT_Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:45:49,384] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-5d71bde4-301c-4a99-904b-f47c0b8df4d5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:45:49,384] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:45:49,384] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:45:49,739] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:45:49,740] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:45:49,740] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:45:49,740] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:45:49,742] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:45:49,742] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:45:49,744] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:45:49,744] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 66 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=85, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:45:49,744] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 85 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:45:49,744] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:45:49,744] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:45:49,744] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:45:49,746] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=67, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:45:49,748] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=67, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:45:49,748] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 67 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=85, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:45:49,748] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 85 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:45:49,749] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:46:18,414] INFO 87.208.3.218 - - [13/Apr/2025:14:46:18 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 404 69 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:46:27,757] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:46:27,761] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:46:27,763] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:46:27,763] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:46:27,764] INFO 87.208.3.218 - - [13/Apr/2025:14:46:27 +0000] "POST /connectors HTTP/1.1" 201 780 "-" "PostmanRuntime/7.43.3" 12 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:46:27,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=68, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:46:27,765] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=68, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:46:27,766] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 68 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=86, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:46:27,766] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 86 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:46:27,766] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:46:27,766] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:46:27,766] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:46:27,767] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:46:27,767] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:46:27,767] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:46:27,767] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:46:27,768] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:46:27,768] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:46:27,768] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:46:27,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:46:27,778] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:46:27,778] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:46:27,780] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=69, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:46:27,781] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=69, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:46:27,781] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 69 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=88, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:46:27,782] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 88 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:46:27,782] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:46:27,782] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:46:27,782] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:46:27,782] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:46:27,782] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:46:27,783] WARN [sink-to-database02|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:101)
[2025-04-13 14:46:27,783] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:46:27,784] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:46:27,784] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:46:27,784] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:46:27,784] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:46:27,786] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:46:27,786] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:46:27,786] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:46:27,786] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744555587786 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:46:27,787] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:46:27,787] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:46:27,787] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:46:27,788] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:46:27,789] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:27,789] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:46:27,789] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:46:27,796] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:46:27,796] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:46:27,798] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:46:27,800] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-2fc3cda4-40ad-4a95-ac2c-8752efeda7c5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:46:27,800] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:46:30,801] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-2fc3cda4-40ad-4a95-ac2c-8752efeda7c5', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:46:30,802] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 3: {connector-consumer-sink-to-database02-0-2fc3cda4-40ad-4a95-ac2c-8752efeda7c5=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:46:30,803] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-2fc3cda4-40ad-4a95-ac2c-8752efeda7c5', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:46:30,803] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:46:30,803] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:46:30,804] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:46:30,807] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:46:30,822] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:30,883] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:30,923] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:30,925] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:30,925] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:30,925] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:30,925] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:33,925] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:33,969] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:34,014] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:34,017] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:34,017] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:34,017] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:34,017] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:37,017] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:37,057] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:37,090] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:37,092] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:37,092] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:37,093] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:37,093] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:37,365] INFO 87.208.3.218 - - [13/Apr/2025:14:46:37 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:46:40,093] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:40,127] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:40,185] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:40,187] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:40,187] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:40,187] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:40,187] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:43,188] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:43,236] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:43,288] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:43,290] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:43,290] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:43,290] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:43,290] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:46,291] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:46,323] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:46,366] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:46,368] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:46,369] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:46,369] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:46,369] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:49,369] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:49,414] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:49,445] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:49,447] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:49,448] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:49,448] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:49,448] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:52,448] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:52,481] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:52,517] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:52,519] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:52,519] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:52,519] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:52,519] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:55,519] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:55,560] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:55,593] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:55,595] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:55,595] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:55,595] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:55,595] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:46:58,596] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:46:58,634] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:46:58,672] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:46:58,674] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:46:58,674] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:46:58,674] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:46:58,674] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:47:01,674] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:47:06,729] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:47:06,762] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:47:06,765] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:47:06,765] ERROR [sink-to-database02|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 14:47:06,765] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:47:06,765] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] The pause flag in partitions [azure-sql-.database01.SalesLT.Product-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-2fc3cda4-40ad-4a95-ac2c-8752efeda7c5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:47:06,766] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:47:06,769] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:47:06,769] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:47:06,769] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:47:06,769] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:47:06,772] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:48:25,586] INFO 87.208.3.218 - - [13/Apr/2025:14:48:25 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2167 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:48:36,829] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:48:36,829] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:48:36,829] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:48:36,830] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:48:36,830] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:48:36,830] INFO 87.208.3.218 - - [13/Apr/2025:14:48:36 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:48:36,830] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:48:36,832] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:48:36,832] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:48:36,833] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=70, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:48:36,835] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=70, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:48:36,835] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:48:36,835] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:48:36,835] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:48:36,835] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:48:36,836] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:48:36,837] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:48:36,837] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 70 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=90, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:48:36,838] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 90 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:48:36,838] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:48:36,838] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:48:36,838] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:48:36,844] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=71, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:48:36,845] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=71, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:48:36,845] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 71 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=90, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:48:36,845] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 90 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:48:36,845] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:49:09,124] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:51:16,163] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:51:16,165] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:51:16,167] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:51:16,167] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:51:16,168] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=72, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:51:16,168] INFO 87.208.3.218 - - [13/Apr/2025:14:51:16 +0000] "POST /connectors HTTP/1.1" 201 591 "-" "PostmanRuntime/7.43.3" 8 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:51:16,170] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=72, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:51:16,171] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 72 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=91, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:51:16,171] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 91 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:51:16,171] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:51:16,171] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:51:16,171] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:51:16,172] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:51:16,172] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:51:16,173] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:51:16,173] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:51:16,173] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:51:16,173] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:51:16,173] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:51:16,183] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:51:16,183] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:51:16,183] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:51:16,184] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=73, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:51:16,187] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=73, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:51:16,187] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 73 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=93, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:51:16,187] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 93 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:51:16,188] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:51:16,188] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_database01_SalesLT_Product]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:51:16,189] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:51:16,190] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:51:16,193] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:51:16,193] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:51:16,193] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:51:16,193] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744555876193 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:51:16,194] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure_sql_database01_SalesLT_Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:51:16,194] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:51:16,194] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:51:16,194] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:51:16,194] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:51:16,194] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:51:16,196] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:51:16,197] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:51:16,198] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:51:16,199] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:51:16,201] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-0c1743db-afe6-4b6c-b007-25bb0cebcec9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:51:16,201] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:51:19,203] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-0c1743db-afe6-4b6c-b007-25bb0cebcec9', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:51:19,203] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-0-0c1743db-afe6-4b6c-b007-25bb0cebcec9=Assignment(partitions=[azure_sql_database01_SalesLT_Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:51:19,206] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-0c1743db-afe6-4b6c-b007-25bb0cebcec9', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:51:19,206] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure_sql_database01_SalesLT_Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:51:19,206] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure_sql_database01_SalesLT_Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:51:19,207] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure_sql_database01_SalesLT_Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:51:19,210] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure_sql_database01_SalesLT_Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:51:38,027] INFO 87.208.3.218 - - [13/Apr/2025:14:51:38 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:52:00,685] INFO [azure-sql-server-source|task-0] 5 records sent during previous 00:25:40.312, last recorded offset of {server=azure-sql-, database=database01} partition is {transaction_id=null, event_serial_no=1, commit_lsn=00000112:00000868:0024, change_lsn=00000112:00000868:0002} (io.debezium.connector.common.BaseSourceTask:213)
[2025-04-13 14:52:06,195] INFO 87.208.3.218 - - [13/Apr/2025:14:52:06 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:52:07,731] INFO 87.208.3.218 - - [13/Apr/2025:14:52:07 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:52:09,126] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 14:52:09,369] INFO 87.208.3.218 - - [13/Apr/2025:14:52:09 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:52:29,605] INFO 87.208.3.218 - - [13/Apr/2025:14:52:29 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:56:29,969] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 14:56:29,969] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 14:56:29,972] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 14:56:29,972] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:56:29,972] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 14:56:29,973] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 14:56:29,973] INFO 87.208.3.218 - - [13/Apr/2025:14:56:29 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 8 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:56:29,973] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:56:29,973] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:56:29,974] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=74, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:56:29,976] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=74, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:56:29,976] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 14:56:29,977] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 14:56:29,977] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 14:56:29,977] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 14:56:29,977] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:56:29,977] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure_sql_database01_SalesLT_Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:56:29,977] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-0c1743db-afe6-4b6c-b007-25bb0cebcec9 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:56:29,978] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:56:29,978] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:56:30,192] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:56:30,192] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:56:30,192] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:56:30,192] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:56:30,194] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:56:30,195] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 14:56:30,197] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 14:56:30,197] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 74 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=95, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:56:30,198] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 95 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:56:30,198] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:56:30,198] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:56:30,198] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:56:30,200] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=75, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:56:30,203] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=75, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:56:30,203] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 75 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=95, connectorIds=[azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:56:30,203] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 95 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:56:30,203] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:56:36,520] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 14:56:36,523] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 14:56:36,523] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:56:36,523] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:56:36,524] INFO 87.208.3.218 - - [13/Apr/2025:14:56:36 +0000] "POST /connectors HTTP/1.1" 201 974 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:56:36,526] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=76, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:56:36,528] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=76, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:56:36,528] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 76 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=96, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:56:36,528] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 96 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:56:36,528] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 14:56:36,528] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 14:56:36,529] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix, unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:56:36,529] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix, unwrap]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:56:36,529] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 14:56:36,529] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 14:56:36,529] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:56:36,530] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix, unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:56:36,530] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix, unwrap]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:56:36,531] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 14:56:36,536] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 14:56:36,537] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 14:56:36,537] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 14:56:36,538] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=77, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 14:56:36,540] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=77, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 14:56:36,540] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 77 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=98, connectorIds=[sink-to-database02, azure-sql-server-source], taskIds=[sink-to-database02-0, azure-sql-server-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 14:56:36,540] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 98 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 14:56:36,540] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 14:56:36,540] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 14:56:36,541] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [dropPrefix, unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 14:56:36,541] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [dropPrefix, unwrap]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:56:36,541] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 14:56:36,541] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 14:56:36,541] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:56:36,541] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 14:56:36,542] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 14:56:36,542] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 14:56:36,542] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 14:56:36,542] WARN [sink-to-database02|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:101)
[2025-04-13 14:56:36,542] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.RegexRouter, io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 14:56:36,542] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix, unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 14:56:36,543] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure-sql-.database01.SalesLT.Product]
	topics.regex = 
	transforms = [dropPrefix, unwrap]
	transforms.dropPrefix.negate = false
	transforms.dropPrefix.predicate = null
	transforms.dropPrefix.regex = azure-sql-\.database01\.(.*)
	transforms.dropPrefix.replacement = $1
	transforms.dropPrefix.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 14:56:36,543] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 14:56:36,543] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 14:56:36,545] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 14:56:36,545] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 14:56:36,545] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 14:56:36,545] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744556196545 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 14:56:36,546] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure-sql-.database01.SalesLT.Product (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 14:56:36,546] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 14:56:36,546] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 14:56:36,546] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:36,546] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 14:56:36,546] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 14:56:36,549] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 14:56:36,551] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 14:56:36,552] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 14:56:36,554] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:56:36,557] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-78bba215-e6ea-48c2-968b-3d80789462bd (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:56:36,557] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 14:56:39,558] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-78bba215-e6ea-48c2-968b-3d80789462bd', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 14:56:39,559] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 3: {connector-consumer-sink-to-database02-0-78bba215-e6ea-48c2-968b-3d80789462bd=Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 14:56:39,560] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-0-78bba215-e6ea-48c2-968b-3d80789462bd', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 14:56:39,560] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure-sql-.database01.SalesLT.Product-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 14:56:39,560] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 14:56:39,561] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 14:56:39,562] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure-sql-.database01.SalesLT.Product-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 14:56:39,635] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:39,698] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:39,733] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:39,735] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:39,736] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:39,736] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:39,736] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:56:42,737] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:42,768] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:42,801] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:42,802] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:42,803] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:42,803] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:42,803] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:56:45,230] INFO 87.208.3.218 - - [13/Apr/2025:14:56:45 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 4 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:56:45,804] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:45,846] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:45,884] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:45,886] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:45,886] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:45,886] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:45,886] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:56:48,887] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:48,917] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:48,948] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:48,949] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:48,949] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:48,950] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:48,950] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:56:51,951] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:51,997] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:52,031] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:52,033] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:52,033] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:52,033] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:52,033] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:56:55,034] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:55,071] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:55,103] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:55,105] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:55,105] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:55,105] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:55,105] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:56:58,106] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:56:58,142] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:56:58,177] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:56:58,179] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:56:58,179] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:56:58,179] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:56:58,179] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:57:01,072] INFO 87.208.3.218 - - [13/Apr/2025:14:57:01 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 168 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:57:01,179] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:57:01,216] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:57:01,253] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:57:01,255] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:57:01,255] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:57:01,256] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:57:01,256] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:57:04,256] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:57:04,291] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:57:04,322] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:57:04,323] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:57:04,323] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:57:04,324] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:57:04,324] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:57:07,325] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:57:07,357] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:57:07,389] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:57:07,390] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:57:07,390] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:57:07,390] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 14:57:07,391] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:57:10,391] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 14:57:10,442] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 14:57:10,482] INFO [sink-to-database02|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 14:57:10,484] WARN [sink-to-database02|task-0] Write of 198 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 14:57:10,484] ERROR [sink-to-database02|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 14:57:10,484] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:57:10,484] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 14:57:10,484] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 14:57:10,484] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 14:57:10,485] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure-sql-.database01.SalesLT.Product-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 14:57:10,485] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] The pause flag in partitions [azure-sql-.database01.SalesLT.Product-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 14:57:10,485] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-78bba215-e6ea-48c2-968b-3d80789462bd sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 14:57:10,485] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 14:57:10,485] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 14:57:10,487] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 14:57:10,487] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:57:10,487] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 14:57:10,487] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 14:57:10,490] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 14:57:31,733] INFO 87.208.3.218 - - [13/Apr/2025:14:57:31 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2167 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 14:58:09,130] INFO [azure-sql-server-source|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)

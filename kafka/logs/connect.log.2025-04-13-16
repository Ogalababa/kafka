[2025-04-13 16:00:03,805] INFO 87.208.3.218 - - [13/Apr/2025:16:00:03 +0000] "GET /connectors/azure-sql-server-source-adres/status HTTP/1.1" 404 90 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:00:04,818] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 6 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:09,819] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 7 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:10,328] INFO 87.208.3.218 - - [13/Apr/2025:16:00:10 +0000] "GET /connectors/azure-sql-server-source-adress/status HTTP/1.1" 200 128 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:00:14,820] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 8 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:19,821] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 9 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:22,092] INFO 87.208.3.218 - - [13/Apr/2025:16:00:22 +0000] "GET /connectors/azure-sql-server-source/status HTTP/1.1" 200 175 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:00:24,821] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 10 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:27,690] INFO 87.208.3.218 - - [13/Apr/2025:16:00:27 +0000] "GET /connectors/azure-sql-server-source-adress/status HTTP/1.1" 200 128 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:00:29,822] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 11 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:34,822] ERROR [azure-sql-server-source-adress|task-0] Failed to register metrics MBean, metrics will not be available (io.debezium.pipeline.JmxUtils:62)
[2025-04-13 16:00:34,823] INFO [azure-sql-server-source-adress|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:586)
[2025-04-13 16:00:34,823] INFO [azure-sql-server-source-adress|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = azure-sql--schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:00:34,823] INFO [azure-sql-server-source-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:00:34,827] INFO [azure-sql-server-source-adress|task-0] The mbean of App info: [kafka.producer], id: [azure-sql--schemahistory] already exists, so skipping a new mbean creation. (org.apache.kafka.common.utils.AppInfoParser:65)
[2025-04-13 16:00:34,829] INFO [azure-sql-server-source-adress|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = azure-sql--schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = azure-sql--schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:00:34,829] INFO [azure-sql-server-source-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:00:34,833] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=azure-sql--schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:00:34,833] INFO [azure-sql-server-source-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:00:34,834] INFO [azure-sql-server-source-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:00:34,834] INFO [azure-sql-server-source-adress|task-0] Kafka startTimeMs: 1744560034833 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:00:34,837] INFO [azure-sql-server-source-adress|task-0] [Consumer clientId=azure-sql--schemahistory, groupId=azure-sql--schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:00:34,838] INFO [azure-sql-server-source-adress|task-0] [Consumer clientId=azure-sql--schemahistory, groupId=azure-sql--schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:00:34,838] INFO [azure-sql-server-source-adress|task-0] [Consumer clientId=azure-sql--schemahistory, groupId=azure-sql--schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:00:34,838] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:00:34,838] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:00:34,838] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:00:34,838] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:00:34,839] INFO [azure-sql-server-source-adress|task-0] App info kafka.consumer for azure-sql--schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:00:34,846] INFO [azure-sql-server-source-adress|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:378)
[2025-04-13 16:00:34,847] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure-sql- named = SignalProcessor (io.debezium.util.Threads:271)
[2025-04-13 16:00:34,850] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure-sql- named = change-event-source-coordinator (io.debezium.util.Threads:271)
[2025-04-13 16:00:34,851] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure-sql- named = blocking-snapshot (io.debezium.util.Threads:271)
[2025-04-13 16:00:34,851] INFO [azure-sql-server-source-adress|task-0] Creating thread debezium-sqlserverconnector-azure-sql--change-event-source-coordinator (io.debezium.util.Threads:288)
[2025-04-13 16:00:34,852] INFO [azure-sql-server-source-adress|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-04-13 16:00:34,852] INFO [azure-sql-server-source-adress|task-0] Creating thread debezium-sqlserverconnector-azure-sql--SignalProcessor (io.debezium.util.Threads:288)
[2025-04-13 16:00:34,852] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 1 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:34,854] INFO [azure-sql-server-source-adress|task-0] WorkerSourceTask{id=azure-sql-server-source-adress-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:279)
[2025-04-13 16:00:39,853] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 2 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:44,853] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 3 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:49,854] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 4 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:54,855] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 5 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:00:55,365] INFO 87.208.3.218 - - [13/Apr/2025:16:00:55 +0000] "GET /connectors/azure-sql-server-source-adress/status HTTP/1.1" 200 182 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:00:59,855] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 6 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:04,856] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 7 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:09,857] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 8 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:14,857] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 9 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:19,858] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 10 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:24,858] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 11 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:29,859] ERROR [azure-sql-server-source-adress|task-0] Failed to register metrics MBean, metrics will not be available (io.debezium.pipeline.JmxUtils:62)
[2025-04-13 16:01:29,859] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 1 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:34,859] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 2 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:39,860] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 3 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:44,861] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 4 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:49,861] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 5 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:54,862] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 6 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:01:55,394] ERROR Uncaught exception in REST call to / (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:64)
javax.ws.rs.NotAllowedException: HTTP 405 Method Not Allowed
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.getMethodRouter(MethodSelectingRouter.java:408)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.access$000(MethodSelectingRouter.java:73)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter$4.apply(MethodSelectingRouter.java:673)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.apply(MethodSelectingRouter.java:304)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:191)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:01:55,395] INFO 45.158.77.35 - - [13/Apr/2025:16:01:55 +0000] "CONNECT example.com:443 HTTP/1.1" 405 58 "-" "Go-http-client/1.1" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:01:59,862] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 7 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:04,863] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 8 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:09,864] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 9 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:14,864] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 10 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:19,865] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 11 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:24,865] ERROR [azure-sql-server-source-adress|task-0] Failed to register metrics MBean, metrics will not be available (io.debezium.pipeline.JmxUtils:62)
[2025-04-13 16:02:24,865] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 1 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:29,866] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 2 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:34,867] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 3 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:39,867] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 4 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:44,868] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 5 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:49,868] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 6 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:54,869] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 7 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:02:59,869] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 8 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:04,870] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 9 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:09,870] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 10 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:14,871] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 11 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:16,674] INFO 87.208.3.218 - - [13/Apr/2025:16:03:16 +0000] "GET /connectors HTTP/1.1" 200 60 "-" "PostmanRuntime/7.43.3" 1 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:03:19,871] ERROR [azure-sql-server-source-adress|task-0] Failed to register metrics MBean, metrics will not be available (io.debezium.pipeline.JmxUtils:62)
[2025-04-13 16:03:19,872] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 1 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:24,872] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 2 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:29,873] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 3 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:34,873] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 4 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:39,874] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 5 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:40,425] INFO Successfully processed removal of connector 'azure-sql-server-source' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:03:40,425] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:03:40,425] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector azure-sql-server-source (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:03:40,425] INFO [azure-sql-server-source|worker] Stopping connector azure-sql-server-source (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:03:40,425] INFO [azure-sql-server-source|worker] Scheduled shutdown for WorkerConnector{id=azure-sql-server-source} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:03:40,426] INFO 87.208.3.218 - - [13/Apr/2025:16:03:40 +0000] "DELETE /connectors/azure-sql-server-source/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:03:40,426] INFO [azure-sql-server-source|worker] Completed shutdown for WorkerConnector{id=azure-sql-server-source} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:03:40,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:03:40,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:03:40,429] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=82, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:03:40,431] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=82, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:03:40,431] INFO [azure-sql-server-source|worker] Stopping connector azure-sql-server-source (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:03:40,431] WARN [azure-sql-server-source|worker] Ignoring stop request for unowned connector azure-sql-server-source (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:03:40,431] WARN [azure-sql-server-source|worker] Ignoring await stop request for non-present connector azure-sql-server-source (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:03:40,431] INFO [azure-sql-server-source|task-0] Stopping task azure-sql-server-source-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:03:41,550] INFO [azure-sql-server-source|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:282)
[2025-04-13 16:03:44,874] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 6 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:45,431] ERROR [azure-sql-server-source|task-0] Graceful stop of task azure-sql-server-source-0 failed. (org.apache.kafka.connect.runtime.Worker:1074)
[2025-04-13 16:03:45,432] INFO [Producer clientId=connector-producer-azure-sql-server-source-0] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:03:45,433] INFO [Producer clientId=connector-producer-azure-sql-server-source-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1407)
[2025-04-13 16:03:45,434] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:03:45,437] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:03:45,437] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:45,438] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:45,438] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:03:45,438] INFO App info kafka.producer for connector-producer-azure-sql-server-source-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:03:45,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:03:45,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 82 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=106, connectorIds=[azure-sql-server-source-adress], taskIds=[azure-sql-server-source-adress-0], revokedConnectorIds=[azure-sql-server-source], revokedTaskIds=[azure-sql-server-source-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:03:45,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 106 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:03:45,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:03:45,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:03:45,440] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:03:45,443] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=83, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:03:45,445] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=83, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:03:45,445] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 83 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=106, connectorIds=[azure-sql-server-source-adress], taskIds=[azure-sql-server-source-adress-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:03:45,445] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 106 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:03:45,445] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:03:46,752] INFO Successfully processed removal of connector 'azure-sql-server-source-adress' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:03:46,752] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source-adress config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:03:46,752] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:03:46,752] INFO [azure-sql-server-source-adress|worker] Stopping connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:03:46,752] INFO [azure-sql-server-source-adress|worker] Scheduled shutdown for WorkerConnector{id=azure-sql-server-source-adress} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:03:46,752] INFO [azure-sql-server-source-adress|worker] Completed shutdown for WorkerConnector{id=azure-sql-server-source-adress} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:03:46,752] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:03:46,753] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:03:46,753] INFO 87.208.3.218 - - [13/Apr/2025:16:03:46 +0000] "DELETE /connectors/azure-sql-server-source-adress/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:03:46,754] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=84, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:03:46,756] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=84, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:03:46,756] INFO [azure-sql-server-source-adress|worker] Stopping connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:03:46,756] WARN [azure-sql-server-source-adress|worker] Ignoring stop request for unowned connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:03:46,756] INFO [azure-sql-server-source-adress|task-0] Stopping task azure-sql-server-source-adress-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:03:46,756] WARN [azure-sql-server-source-adress|worker] Ignoring await stop request for non-present connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:03:49,876] WARN [azure-sql-server-source-adress|task-0] Unable to register metrics as an old set with the same name exists, retrying in PT5S (attempt 7 out of 12) (io.debezium.pipeline.JmxUtils:55)
[2025-04-13 16:03:49,893] INFO [azure-sql-server-source-adress|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:282)
[2025-04-13 16:03:50,085] INFO [azure-sql-server-source|task-0] Finished streaming (io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator:133)
[2025-04-13 16:03:50,085] INFO [azure-sql-server-source|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:03:50,086] INFO [azure-sql-server-source|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-04-13 16:03:50,087] INFO [azure-sql-server-source|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-04-13 16:03:50,089] INFO [azure-sql-server-source|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:03:50,089] INFO [azure-sql-server-source|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:03:50,089] INFO [azure-sql-server-source|task-0] [Producer clientId=azure-sql--schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:03:50,091] INFO [azure-sql-server-source|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:03:50,091] INFO [azure-sql-server-source|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:50,091] INFO [azure-sql-server-source|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:50,091] INFO [azure-sql-server-source|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] App info kafka.producer for azure-sql--schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] [Producer clientId=connector-producer-azure-sql-server-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:03:50,092] INFO [azure-sql-server-source|task-0] App info kafka.producer for connector-producer-azure-sql-server-source-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:03:51,757] ERROR [azure-sql-server-source-adress|task-0] Graceful stop of task azure-sql-server-source-adress-0 failed. (org.apache.kafka.connect.runtime.Worker:1074)
[2025-04-13 16:03:51,757] INFO [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:03:51,758] INFO [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1407)
[2025-04-13 16:03:51,758] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:03:51,758] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:03:51,758] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 84 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=108, connectorIds=[], taskIds=[], revokedConnectorIds=[azure-sql-server-source-adress], revokedTaskIds=[azure-sql-server-source-adress-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:03:51,758] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 108 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:03:51,759] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:03:51,759] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:03:51,759] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:03:51,760] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:03:51,760] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:51,760] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:51,760] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:03:51,761] INFO App info kafka.producer for connector-producer-azure-sql-server-source-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:03:51,763] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=85, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:03:51,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=85, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:03:51,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 85 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=108, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:03:51,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 108 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:03:51,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:03:52,550] INFO 87.208.3.218 - - [13/Apr/2025:16:03:52 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:03:54,877] INFO [azure-sql-server-source-adress|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:131)
[2025-04-13 16:03:54,877] INFO [azure-sql-server-source-adress|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:134)
[2025-04-13 16:03:54,877] INFO [azure-sql-server-source-adress|task-0] No previous offset has been found (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:78)
[2025-04-13 16:03:54,878] INFO [azure-sql-server-source-adress|task-0] According to the connector configuration both schema and data will be snapshotted (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:80)
[2025-04-13 16:03:54,878] INFO [azure-sql-server-source-adress|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:121)
[2025-04-13 16:03:54,945] INFO [azure-sql-server-source-adress|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:130)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductCategory to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.dbo.BuildVersion to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductModelProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductModel to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.SalesOrderHeader to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.Address to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.Customer to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.SalesOrderDetail to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.dbo.ErrorLog to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.CustomerAddress to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.Product to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:222)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Snapshot step 3 - Locking captured tables [database01.SalesLT.Address] (io.debezium.relational.RelationalSnapshotChangeEventSource:139)
[2025-04-13 16:03:54,951] INFO [azure-sql-server-source-adress|task-0] Setting locking timeout to 10 s (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:135)
[2025-04-13 16:03:54,959] INFO [azure-sql-server-source-adress|task-0] Executing schema locking (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:140)
[2025-04-13 16:03:54,959] ERROR [azure-sql-server-source-adress|task-0] Error during snapshot (io.debezium.relational.RelationalSnapshotChangeEventSource:179)
java.lang.InterruptedException: Interrupted while locking table database01.SalesLT.Address
	at io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource.lockTablesForSchemaSnapshot(SqlServerSnapshotChangeEventSource.java:144)
	at io.debezium.relational.RelationalSnapshotChangeEventSource.doExecute(RelationalSnapshotChangeEventSource.java:142)
	at io.debezium.pipeline.source.AbstractSnapshotChangeEventSource.execute(AbstractSnapshotChangeEventSource.java:92)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:253)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:237)
	at io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator.executeChangeEventSources(SqlServerChangeEventSourceCoordinator.java:82)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.lambda$start$0(ChangeEventSourceCoordinator.java:137)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:03:54,965] WARN [azure-sql-server-source-adress|task-0] Snapshot was interrupted before completion (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:96)
[2025-04-13 16:03:54,965] INFO [azure-sql-server-source-adress|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:104)
[2025-04-13 16:03:54,965] WARN [azure-sql-server-source-adress|task-0] Snapshot was not completed successfully, it will be re-executed upon connector restart (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:115)
[2025-04-13 16:03:54,968] INFO [azure-sql-server-source-adress|task-0] Removing locking timeout (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:262)
[2025-04-13 16:03:54,971] WARN [azure-sql-server-source-adress|task-0] Change event source executor was interrupted (io.debezium.pipeline.ChangeEventSourceCoordinator:141)
java.lang.InterruptedException: Interrupted while locking table database01.SalesLT.Address
	at io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource.lockTablesForSchemaSnapshot(SqlServerSnapshotChangeEventSource.java:144)
	at io.debezium.relational.RelationalSnapshotChangeEventSource.doExecute(RelationalSnapshotChangeEventSource.java:142)
	at io.debezium.pipeline.source.AbstractSnapshotChangeEventSource.execute(AbstractSnapshotChangeEventSource.java:92)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:253)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:237)
	at io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator.executeChangeEventSources(SqlServerChangeEventSourceCoordinator.java:82)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.lambda$start$0(ChangeEventSourceCoordinator.java:137)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:03:54,971] INFO [azure-sql-server-source-adress|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:03:54,972] INFO [azure-sql-server-source-adress|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-04-13 16:03:54,972] INFO [azure-sql-server-source-adress|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-04-13 16:03:54,972] INFO [azure-sql-server-source-adress|task-0] Unable to unregister metrics MBean 'debezium.sql_server:type=connector-metrics,server=azure-sql-,task=0,context=snapshot,database=database01' as it was not found (io.debezium.pipeline.JmxUtils:97)
[2025-04-13 16:03:54,972] INFO [azure-sql-server-source-adress|task-0] Unable to unregister metrics MBean 'debezium.sql_server:type=connector-metrics,server=azure-sql-,task=0,context=snapshot' as it was not found (io.debezium.pipeline.JmxUtils:97)
[2025-04-13 16:03:54,972] INFO [azure-sql-server-source-adress|task-0] Unable to unregister metrics MBean 'debezium.sql_server:type=connector-metrics,server=azure-sql-,task=0,context=streaming' as it was not found (io.debezium.pipeline.JmxUtils:97)
[2025-04-13 16:03:54,973] INFO [azure-sql-server-source-adress|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:03:54,973] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=azure-sql--schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] App info kafka.producer for azure-sql--schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Unable to unregister metrics MBean 'debezium.sql_server:type=connector-metrics,server=azure-sql-,task=0,context=schema-history' as it was not found (io.debezium.pipeline.JmxUtils:97)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:03:54,974] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:54,975] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:03:54,975] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:03:54,975] INFO [azure-sql-server-source-adress|task-0] App info kafka.producer for connector-producer-azure-sql-server-source-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:09:42,278] INFO Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:09:42,340] INFO Checking if user has access to CDC table (io.debezium.connector.sqlserver.SqlServerConnector:128)
[2025-04-13 16:09:42,356] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:09:42,357] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:09:42,361] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source-adress config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:09:42,361] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:09:42,361] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:09:42,362] INFO 87.208.3.218 - - [13/Apr/2025:16:09:42 +0000] "POST /connectors HTTP/1.1" 201 999 "-" "PostmanRuntime/7.43.3" 87 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:09:42,363] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=86, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:09:42,366] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=86, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:09:42,366] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 86 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=109, connectorIds=[azure-sql-server-source-adress], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:09:42,366] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 109 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:09:42,367] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:09:42,367] INFO [azure-sql-server-source-adress|worker] Creating connector azure-sql-server-source-adress of type io.debezium.connector.sqlserver.SqlServerConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:09:42,367] INFO [azure-sql-server-source-adress|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:09:42,367] INFO [azure-sql-server-source-adress|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:09:42,368] INFO [azure-sql-server-source-adress|worker] Instantiated connector azure-sql-server-source-adress with version 2.5.1.Final of type class io.debezium.connector.sqlserver.SqlServerConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:09:42,368] INFO [azure-sql-server-source-adress|worker] Finished creating connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:09:42,368] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:09:42,369] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:09:42,369] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:09:42,369] INFO [azure-sql-server-source-adress|worker] Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:09:42,414] INFO [azure-sql-server-source-adress|worker] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:09:42,424] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [azure-sql-server-source-adress-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:09:42,424] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:09:42,425] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:09:42,426] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=87, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:09:42,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=87, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:09:42,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 87 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=111, connectorIds=[azure-sql-server-source-adress], taskIds=[azure-sql-server-source-adress-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:09:42,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 111 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:09:42,427] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task azure-sql-server-source-adress-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:09:42,428] INFO [azure-sql-server-source-adress|task-0] Creating task azure-sql-server-source-adress-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:09:42,428] INFO [azure-sql-server-source-adress|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:09:42,428] INFO [azure-sql-server-source-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:09:42,428] INFO [azure-sql-server-source-adress|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.sqlserver.SqlServerConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:09:42,429] INFO [azure-sql-server-source-adress|task-0] Instantiated task azure-sql-server-source-adress-0 with version 2.5.1.Final of type io.debezium.connector.sqlserver.SqlServerConnectorTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:09:42,429] INFO [azure-sql-server-source-adress|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:09:42,429] INFO [azure-sql-server-source-adress|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task azure-sql-server-source-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:09:42,430] INFO [azure-sql-server-source-adress|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:09:42,430] INFO [azure-sql-server-source-adress|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task azure-sql-server-source-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:09:42,430] INFO [azure-sql-server-source-adress|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task azure-sql-server-source-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:09:42,430] WARN [azure-sql-server-source-adress|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:101)
[2025-04-13 16:09:42,430] INFO [azure-sql-server-source-adress|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:09:42,430] INFO [azure-sql-server-source-adress|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:09:42,430] INFO [azure-sql-server-source-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-adress
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:09:42,431] INFO [azure-sql-server-source-adress|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-azure-sql-server-source-adress-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:09:42,431] INFO [azure-sql-server-source-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:09:42,433] INFO [azure-sql-server-source-adress|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:381)
[2025-04-13 16:09:42,434] INFO [azure-sql-server-source-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:09:42,434] INFO [azure-sql-server-source-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:09:42,434] INFO [azure-sql-server-source-adress|task-0] Kafka startTimeMs: 1744560582434 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:09:42,434] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0] Starting SqlServerConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:135)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    connector.class = io.debezium.connector.sqlserver.SqlServerConnector (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    incrementing.column.name = AddressID (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    transforms.unwrap.delete.handling.mode = rewrite (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    mode = incrementing (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    topic.prefix = azure_sql_ (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    schema.history.internal.kafka.topic = schema-changes.database01 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    poll.interval.ms = 10000 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    transforms.unwrap.drop.tombstones = true (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    database.encrypt = true (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    database.user = kafkadmin@kafkadatabase (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,435] INFO [azure-sql-server-source-adress|task-0]    database.dbname = database01 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    database.names = database01 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    database.server.name = server1 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    schema.history.internal.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    database.port = 1433 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    task.class = io.debezium.connector.sqlserver.SqlServerConnectorTask (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    database.hostname = kafkadatabase.database.windows.net (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    name = azure-sql-server-source-adress (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    task.id = 0 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    table.include.list = SalesLT.Address (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0]    database.trustServerCertificate = true (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:09:42,436] INFO [azure-sql-server-source-adress|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.SchemaTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1066)
[2025-04-13 16:09:42,439] INFO [azure-sql-server-source-adress|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=azure_sql_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=azure_sql_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-04-13 16:09:42,439] INFO [azure-sql-server-source-adress|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=azure_sql_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-04-13 16:09:42,439] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = db-history-config-check (io.debezium.util.Threads:271)
[2025-04-13 16:09:42,439] INFO [azure-sql-server-source-adress|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:586)
[2025-04-13 16:09:42,439] INFO [azure-sql-server-source-adress|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:09:42,439] INFO [azure-sql-server-source-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:09:42,441] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:09:42,442] INFO [azure-sql-server-source-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:09:42,442] INFO [azure-sql-server-source-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:09:42,442] INFO [azure-sql-server-source-adress|task-0] Kafka startTimeMs: 1744560582442 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:09:42,442] INFO [azure-sql-server-source-adress|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = azure_sql_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:09:42,443] INFO [azure-sql-server-source-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:09:42,445] INFO [azure-sql-server-source-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:09:42,445] INFO [azure-sql-server-source-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:09:42,445] INFO [azure-sql-server-source-adress|task-0] Kafka startTimeMs: 1744560582445 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:09:42,448] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=azure_sql_-schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:09:42,448] INFO [azure-sql-server-source-adress|task-0] [Consumer clientId=azure_sql_-schemahistory, groupId=azure_sql_-schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:09:42,450] INFO [azure-sql-server-source-adress|task-0] [Consumer clientId=azure_sql_-schemahistory, groupId=azure_sql_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:09:42,450] INFO [azure-sql-server-source-adress|task-0] [Consumer clientId=azure_sql_-schemahistory, groupId=azure_sql_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:09:42,450] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:09:42,450] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:09:42,451] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:09:42,451] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:09:42,452] INFO [azure-sql-server-source-adress|task-0] App info kafka.consumer for azure_sql_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:09:42,452] INFO [azure-sql-server-source-adress|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2025-04-13 16:09:42,452] INFO [azure-sql-server-source-adress|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:381)
[2025-04-13 16:09:42,453] INFO [azure-sql-server-source-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:09:42,453] INFO [azure-sql-server-source-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:09:42,453] INFO [azure-sql-server-source-adress|task-0] Kafka startTimeMs: 1744560582453 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:09:42,484] INFO [azure-sql-server-source-adress|task-0] Database schema history topic '(name=schema-changes.database01, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-04-13 16:09:42,487] INFO [azure-sql-server-source-adress|task-0] App info kafka.admin.client for azure_sql_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:09:42,487] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:09:42,487] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:09:42,487] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:09:42,493] INFO [azure-sql-server-source-adress|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:378)
[2025-04-13 16:09:42,495] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = SignalProcessor (io.debezium.util.Threads:271)
[2025-04-13 16:09:42,496] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = change-event-source-coordinator (io.debezium.util.Threads:271)
[2025-04-13 16:09:42,496] INFO [azure-sql-server-source-adress|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = blocking-snapshot (io.debezium.util.Threads:271)
[2025-04-13 16:09:42,496] INFO [azure-sql-server-source-adress|task-0] Creating thread debezium-sqlserverconnector-azure_sql_-change-event-source-coordinator (io.debezium.util.Threads:288)
[2025-04-13 16:09:42,498] INFO [azure-sql-server-source-adress|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-04-13 16:09:42,498] INFO [azure-sql-server-source-adress|task-0] Creating thread debezium-sqlserverconnector-azure_sql_-SignalProcessor (io.debezium.util.Threads:288)
[2025-04-13 16:09:42,499] INFO [azure-sql-server-source-adress|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:131)
[2025-04-13 16:09:42,499] INFO [azure-sql-server-source-adress|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:134)
[2025-04-13 16:09:42,499] INFO [azure-sql-server-source-adress|task-0] No previous offset has been found (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:78)
[2025-04-13 16:09:42,499] INFO [azure-sql-server-source-adress|task-0] According to the connector configuration both schema and data will be snapshotted (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:80)
[2025-04-13 16:09:42,499] INFO [azure-sql-server-source-adress|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:121)
[2025-04-13 16:09:42,503] INFO [azure-sql-server-source-adress|task-0] WorkerSourceTask{id=azure-sql-server-source-adress-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:279)
[2025-04-13 16:09:42,552] INFO [azure-sql-server-source-adress|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:130)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductCategory to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.dbo.BuildVersion to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductModelProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductModel to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.SalesOrderHeader to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.Address to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.ProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.Customer to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.SalesOrderDetail to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.dbo.ErrorLog to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.CustomerAddress to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Adding table database01.SalesLT.Product to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:222)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Snapshot step 3 - Locking captured tables [database01.SalesLT.Address] (io.debezium.relational.RelationalSnapshotChangeEventSource:139)
[2025-04-13 16:09:42,558] INFO [azure-sql-server-source-adress|task-0] Setting locking timeout to 10 s (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:135)
[2025-04-13 16:09:42,568] INFO [azure-sql-server-source-adress|task-0] Executing schema locking (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:140)
[2025-04-13 16:09:42,568] INFO [azure-sql-server-source-adress|task-0] Locking table database01.SalesLT.Address (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:147)
[2025-04-13 16:09:42,571] INFO [azure-sql-server-source-adress|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:145)
[2025-04-13 16:09:42,573] INFO [azure-sql-server-source-adress|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:148)
[2025-04-13 16:09:42,577] INFO [azure-sql-server-source-adress|task-0] Reading structure of schema 'database01' (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:209)
[2025-04-13 16:09:42,593] INFO [azure-sql-server-source-adress|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:152)
[2025-04-13 16:09:42,593] INFO [azure-sql-server-source-adress|task-0] Capturing structure of table database01.SalesLT.Address (io.debezium.relational.RelationalSnapshotChangeEventSource:367)
[2025-04-13 16:09:42,594] WARN [azure-sql-server-source-adress|task-0] Mapper for type 'uniqueidentifier' not found. (io.debezium.connector.sqlserver.SqlServerDefaultValueConverter:68)
[2025-04-13 16:09:42,641] WARN [azure-sql-server-source-adress|task-0] Cannot parse column default value '(getdate())' to type 'datetime'. Expression evaluation is not supported. (io.debezium.connector.sqlserver.SqlServerDefaultValueConverter:78)
[2025-04-13 16:09:42,652] INFO [azure-sql-server-source-adress|task-0] Schema locks released. (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:167)
[2025-04-13 16:09:42,652] INFO [azure-sql-server-source-adress|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:164)
[2025-04-13 16:09:42,652] INFO [azure-sql-server-source-adress|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:414)
[2025-04-13 16:09:42,652] INFO [azure-sql-server-source-adress|task-0] For table 'database01.SalesLT.Address' using select statement: 'SELECT [AddressID], [AddressLine1], [AddressLine2], [City], [StateProvince], [CountryRegion], [PostalCode], [rowguid], [ModifiedDate] FROM [database01].[SalesLT].[Address]' (io.debezium.relational.RelationalSnapshotChangeEventSource:423)
[2025-04-13 16:09:42,653] INFO [azure-sql-server-source-adress|task-0] Exporting data from table 'database01.SalesLT.Address' (1 of 1 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:542)
[2025-04-13 16:09:42,696] INFO [azure-sql-server-source-adress|task-0] 	 Finished exporting 450 records for table 'database01.SalesLT.Address' (1 of 1 tables); total duration '00:00:00.042' (io.debezium.relational.RelationalSnapshotChangeEventSource:588)
[2025-04-13 16:09:42,702] INFO [azure-sql-server-source-adress|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:104)
[2025-04-13 16:09:42,702] INFO [azure-sql-server-source-adress|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-04-13 16:09:42,704] INFO [azure-sql-server-source-adress|task-0] Removing locking timeout (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:262)
[2025-04-13 16:09:42,708] INFO [azure-sql-server-source-adress|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=SqlServerOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.sqlserver.Source:STRUCT}, sourceInfo=SourceInfo [serverName=azure_sql_, changeLsn=NULL, commitLsn=00000114:00000c28:0004, eventSerialNo=null, snapshot=FALSE, sourceTime=2025-04-13T16:09:42.654440Z], snapshotCompleted=true, eventSerialNo=1]] (io.debezium.pipeline.ChangeEventSourceCoordinator:254)
[2025-04-13 16:09:42,708] INFO [azure-sql-server-source-adress|task-0] Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:09:42,708] INFO [azure-sql-server-source-adress|task-0] Starting streaming (io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator:101)
[2025-04-13 16:09:42,708] INFO [azure-sql-server-source-adress|task-0] Last position recorded in offsets is 00000114:00000c28:0004(NULL)[1] (io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource:160)
[2025-04-13 16:09:47,504] INFO [azure-sql-server-source-adress|task-0] 451 records sent during previous 00:00:05.075, last recorded offset of {server=azure_sql_, database=database01} partition is {commit_lsn=00000114:00000c28:0004, snapshot=true, snapshot_completed=true} (io.debezium.connector.common.BaseSourceTask:213)
[2025-04-13 16:09:47,508] WARN [azure-sql-server-source-adress|task-0] [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Error while fetching metadata with correlation id 4 : {azure_sql_=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 16:09:47,632] WARN [azure-sql-server-source-adress|task-0] [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Error while fetching metadata with correlation id 7 : {azure_sql_.database01.SalesLT.Address=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 16:10:42,434] INFO [azure-sql-server-source-adress|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-adress-0} Committing offsets for 451 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 16:12:33,225] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Resetting the last seen epoch of partition schema-changes.database01-0 to 0 since the associated topicId changed from null to tYMsFG5PTrKtvm4K-wyw2A (org.apache.kafka.clients.Metadata:577)
[2025-04-13 16:15:05,875] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2483)
[2025-04-13 16:16:13,224] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:16:13,226] INFO 87.208.3.218 - - [13/Apr/2025:16:16:13 +0000] "POST /connectors HTTP/1.1" 400 408 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:18:42,688] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=azure_sql_-schemahistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:18:47,946] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:20:10,424] INFO 87.208.3.218 - - [13/Apr/2025:16:20:10 +0000] "GET /connectors/azure-sql-server-source-adress/status HTTP/1.1" 200 182 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:23:36,274] INFO [AdminClient clientId=connect-cluster-shared-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:24:46,557] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:24:46,565] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-address config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:24:46,565] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:24:46,566] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:24:46,566] INFO 87.208.3.218 - - [13/Apr/2025:16:24:46 +0000] "POST /connectors HTTP/1.1" 201 796 "-" "PostmanRuntime/7.43.3" 12 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:24:46,567] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=88, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:24:46,568] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=88, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:24:46,568] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 88 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=113, connectorIds=[sink-to-database02-address, azure-sql-server-source-adress], taskIds=[azure-sql-server-source-adress-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:24:46,568] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 113 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:24:46,568] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02-address (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:24:46,568] INFO [sink-to-database02-address|worker] Creating connector sink-to-database02-address of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:24:46,569] INFO [sink-to-database02-address|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:24:46,569] INFO [sink-to-database02-address|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:24:46,569] INFO [sink-to-database02-address|worker] Instantiated connector sink-to-database02-address with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:24:46,569] INFO [sink-to-database02-address|worker] Finished creating connector sink-to-database02-address (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:24:46,570] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:24:46,571] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:24:46,571] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:24:46,571] INFO [sink-to-database02-address|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 16:24:46,579] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-address-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:24:46,580] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:24:46,580] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:24:46,581] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=89, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:24:46,585] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=89, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:24:46,586] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 89 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=115, connectorIds=[sink-to-database02-address, azure-sql-server-source-adress], taskIds=[sink-to-database02-address-0, azure-sql-server-source-adress-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:24:46,586] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 115 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:24:46,586] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-address-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:24:46,586] INFO [sink-to-database02-address|task-0] Creating task sink-to-database02-address-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:24:46,586] INFO [sink-to-database02-address|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:24:46,586] INFO [sink-to-database02-address|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] Instantiated task sink-to-database02-address-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:24:46,587] WARN [sink-to-database02-address|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:101)
[2025-04-13 16:24:46,587] INFO [sink-to-database02-address|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:24:46,588] INFO [sink-to-database02-address|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:24:46,588] INFO [sink-to-database02-address|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:24:46,593] INFO [sink-to-database02-address|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-address-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02-address
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:24:46,593] INFO [sink-to-database02-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:24:46,597] INFO [sink-to-database02-address|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 16:24:46,597] INFO [sink-to-database02-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:24:46,597] INFO [sink-to-database02-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:24:46,597] INFO [sink-to-database02-address|task-0] Kafka startTimeMs: 1744561486597 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:24:46,598] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:24:46,598] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Subscribed to topic(s): azure_sql_.database01.SalesLT.Address (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 16:24:46,598] INFO [sink-to-database02-address|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 16:24:46,598] INFO [sink-to-database02-address|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [AddressID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Address
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 16:24:46,598] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:24:46,599] INFO [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 16:24:46,599] INFO [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 16:24:46,603] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:24:46,605] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 16:24:46,605] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:24:46,610] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-address-0-d3e5db52-65cc-4a15-b12e-2b0d238c3aa7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:24:46,610] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:24:49,613] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-address-0-d3e5db52-65cc-4a15-b12e-2b0d238c3aa7', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 16:24:49,613] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-address-0-d3e5db52-65cc-4a15-b12e-2b0d238c3aa7=Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 16:24:49,615] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-address-0-d3e5db52-65cc-4a15-b12e-2b0d238c3aa7', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 16:24:49,615] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Notifying assignor about the new Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 16:24:49,615] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Adding newly assigned partitions: azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 16:24:49,616] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Found no committed offset for partition azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 16:24:49,617] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Resetting offset for partition azure_sql_.database01.SalesLT.Address-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 16:24:49,659] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:24:49,777] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:24:49,813] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:24:49,815] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:24:49,815] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:24:49,816] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:24:49,816] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:24:52,816] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:24:52,861] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:24:52,895] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:24:52,896] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:24:52,896] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:24:52,897] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:24:52,897] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:24:55,897] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:24:55,936] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:24:55,976] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:24:55,978] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:24:55,978] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:24:55,978] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:24:55,979] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:24:58,979] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:24:59,020] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:24:59,051] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:24:59,054] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:24:59,055] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:24:59,055] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:24:59,055] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:02,056] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:02,095] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:02,127] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:02,130] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:02,130] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:02,130] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:25:02,130] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:05,130] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:05,164] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:05,197] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:05,200] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:05,200] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:05,200] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:25:05,200] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:08,200] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:08,243] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:08,280] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:08,283] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:08,283] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:08,284] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:25:08,284] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:11,285] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:11,323] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:11,358] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:11,360] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:11,360] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:11,360] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:25:11,360] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:14,360] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:14,399] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:14,436] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:14,437] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:14,437] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:14,438] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:25:14,438] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:17,438] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:17,470] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:17,502] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:17,504] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:17,504] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:17,504] INFO [sink-to-database02-address|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:25:17,504] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:19,292] INFO 87.208.3.218 - - [13/Apr/2025:16:25:19 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 404 79 "-" "PostmanRuntime/7.43.3" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:25:20,504] INFO [sink-to-database02-address|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:25:20,558] INFO [sink-to-database02-address|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:25:20,589] INFO [sink-to-database02-address|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Address" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:25:20,590] WARN [sink-to-database02-address|task-0] Write of 450 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:25:20,591] ERROR [sink-to-database02-address|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 16:25:20,591] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:20,591] ERROR [sink-to-database02-address|task-0] WorkerSinkTask{id=sink-to-database02-address-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:25:20,591] INFO [sink-to-database02-address|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 16:25:20,591] INFO [sink-to-database02-address|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:25:20,591] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Revoke previously assigned partitions azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 16:25:20,592] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] The pause flag in partitions [azure_sql_.database01.SalesLT.Address-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 16:25:20,592] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Member connector-consumer-sink-to-database02-address-0-d3e5db52-65cc-4a15-b12e-2b0d238c3aa7 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 16:25:20,592] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:25:20,592] INFO [sink-to-database02-address|task-0] [Consumer clientId=connector-consumer-sink-to-database02-address-0, groupId=connect-sink-to-database02-address] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:25:20,595] INFO [sink-to-database02-address|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:25:20,595] INFO [sink-to-database02-address|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:25:20,595] INFO [sink-to-database02-address|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:25:20,595] INFO [sink-to-database02-address|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:25:20,597] INFO [sink-to-database02-address|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-address-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:25:33,677] INFO 87.208.3.218 - - [13/Apr/2025:16:25:33 +0000] "GET /connectors/sink-to-database02-address/status HTTP/1.1" 200 2175 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:28:42,714] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=azure_sql_-schemahistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:30:12,763] INFO Successfully processed removal of connector 'azure-sql-server-source-adress' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:30:12,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source-adress config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:30:12,764] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:30:12,764] INFO [azure-sql-server-source-adress|worker] Stopping connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:30:12,764] INFO [azure-sql-server-source-adress|worker] Scheduled shutdown for WorkerConnector{id=azure-sql-server-source-adress} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:30:12,764] INFO [azure-sql-server-source-adress|worker] Completed shutdown for WorkerConnector{id=azure-sql-server-source-adress} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:30:12,765] INFO 87.208.3.218 - - [13/Apr/2025:16:30:12 +0000] "DELETE /connectors/azure-sql-server-source-adress/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:30:12,765] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:30:12,765] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:30:12,767] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=90, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:30:12,769] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=90, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:30:12,769] INFO [azure-sql-server-source-adress|worker] Stopping connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:30:12,769] WARN [azure-sql-server-source-adress|worker] Ignoring stop request for unowned connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:30:12,769] INFO [azure-sql-server-source-adress|task-0] Stopping task azure-sql-server-source-adress-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:30:12,769] WARN [azure-sql-server-source-adress|worker] Ignoring await stop request for non-present connector azure-sql-server-source-adress (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:30:13,058] INFO [azure-sql-server-source-adress|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:282)
[2025-04-13 16:30:17,769] ERROR [azure-sql-server-source-adress|task-0] Graceful stop of task azure-sql-server-source-adress-0 failed. (org.apache.kafka.connect.runtime.Worker:1074)
[2025-04-13 16:30:17,770] INFO [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:30:17,770] INFO [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1407)
[2025-04-13 16:30:17,772] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:30:17,772] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:30:17,772] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:30:17,772] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:30:17,772] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:30:17,772] INFO App info kafka.producer for connector-producer-azure-sql-server-source-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:30:17,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:30:17,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 90 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=117, connectorIds=[sink-to-database02-address], taskIds=[sink-to-database02-address-0], revokedConnectorIds=[azure-sql-server-source-adress], revokedTaskIds=[azure-sql-server-source-adress-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:30:17,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 117 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:30:17,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:30:17,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:30:17,776] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:30:17,779] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=91, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:30:17,780] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=91, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:30:17,781] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 91 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=117, connectorIds=[sink-to-database02-address], taskIds=[sink-to-database02-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:30:17,781] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 117 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:30:17,781] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:30:22,708] INFO [azure-sql-server-source-adress|task-0] Finished streaming (io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator:133)
[2025-04-13 16:30:22,708] INFO [azure-sql-server-source-adress|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:30:22,708] INFO [azure-sql-server-source-adress|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-04-13 16:30:22,708] INFO [azure-sql-server-source-adress|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-04-13 16:30:22,709] INFO [azure-sql-server-source-adress|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:30:22,710] INFO [azure-sql-server-source-adress|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:30:22,710] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=azure_sql_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] App info kafka.producer for azure_sql_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] [Producer clientId=connector-producer-azure-sql-server-source-adress-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:30:22,713] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:30:22,714] INFO [azure-sql-server-source-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:30:22,714] INFO [azure-sql-server-source-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:30:22,714] INFO [azure-sql-server-source-adress|task-0] App info kafka.producer for connector-producer-azure-sql-server-source-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:30:34,425] INFO Successfully processed removal of connector 'sink-to-database02-address' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:30:34,425] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-address config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:30:34,425] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02-address (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:30:34,425] INFO [sink-to-database02-address|worker] Stopping connector sink-to-database02-address (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:30:34,425] INFO [sink-to-database02-address|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02-address} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:30:34,426] INFO 87.208.3.218 - - [13/Apr/2025:16:30:34 +0000] "DELETE /connectors/sink-to-database02-address/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:30:34,426] INFO [sink-to-database02-address|worker] Completed shutdown for WorkerConnector{id=sink-to-database02-address} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:30:34,426] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:30:34,426] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:30:34,429] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=92, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:30:34,430] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=92, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:30:34,431] INFO [sink-to-database02-address|worker] Stopping connector sink-to-database02-address (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:30:34,431] WARN [sink-to-database02-address|worker] Ignoring stop request for unowned connector sink-to-database02-address (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:30:34,431] WARN [sink-to-database02-address|worker] Ignoring await stop request for non-present connector sink-to-database02-address (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:30:34,431] INFO [sink-to-database02-address|task-0] Stopping task sink-to-database02-address-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:30:34,431] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:30:34,432] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:30:34,432] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 92 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=119, connectorIds=[], taskIds=[], revokedConnectorIds=[sink-to-database02-address], revokedTaskIds=[sink-to-database02-address-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:30:34,432] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 119 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:30:34,432] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:30:34,432] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:30:34,432] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:30:34,434] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=93, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:30:34,435] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=93, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:30:34,435] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 93 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=119, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:30:34,435] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 119 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:30:34,435] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:30:38,565] INFO 87.208.3.218 - - [13/Apr/2025:16:30:38 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:35:03,691] INFO Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:35:03,760] INFO Checking if user has access to CDC table (io.debezium.connector.sqlserver.SqlServerConnector:128)
[2025-04-13 16:35:03,776] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:35:03,777] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:35:03,780] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source-address config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:35:03,780] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:35:03,781] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:35:03,782] INFO 87.208.3.218 - - [13/Apr/2025:16:35:03 +0000] "POST /connectors HTTP/1.1" 201 882 "-" "PostmanRuntime/7.43.3" 95 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:35:03,782] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=94, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:35:03,784] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=94, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:35:03,784] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 94 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=120, connectorIds=[azure-sql-server-source-address], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:35:03,784] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 120 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:35:03,784] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:35:03,784] INFO [azure-sql-server-source-address|worker] Creating connector azure-sql-server-source-address of type io.debezium.connector.sqlserver.SqlServerConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:35:03,784] INFO [azure-sql-server-source-address|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:35:03,785] INFO [azure-sql-server-source-address|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:35:03,785] INFO [azure-sql-server-source-address|worker] Instantiated connector azure-sql-server-source-address with version 2.5.1.Final of type class io.debezium.connector.sqlserver.SqlServerConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:35:03,785] INFO [azure-sql-server-source-address|worker] Finished creating connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:35:03,785] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:35:03,786] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:35:03,786] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:35:03,787] INFO [azure-sql-server-source-address|worker] Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:35:03,831] INFO [azure-sql-server-source-address|worker] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:35:03,839] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [azure-sql-server-source-address-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:35:03,840] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:35:03,840] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:35:03,841] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=95, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:35:03,842] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=95, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:35:03,842] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 95 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=122, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:35:03,842] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 122 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:35:03,843] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task azure-sql-server-source-address-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:35:03,843] INFO [azure-sql-server-source-address|task-0] Creating task azure-sql-server-source-address-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:35:03,843] INFO [azure-sql-server-source-address|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:35:03,843] INFO [azure-sql-server-source-address|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:35:03,843] INFO [azure-sql-server-source-address|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.sqlserver.SqlServerConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:35:03,844] INFO [azure-sql-server-source-address|task-0] Instantiated task azure-sql-server-source-address-0 with version 2.5.1.Final of type io.debezium.connector.sqlserver.SqlServerConnectorTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task azure-sql-server-source-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task azure-sql-server-source-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task azure-sql-server-source-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:35:03,845] WARN [azure-sql-server-source-address|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:101)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:35:03,845] INFO [azure-sql-server-source-address|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:35:03,846] INFO [azure-sql-server-source-address|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:35:03,846] INFO [azure-sql-server-source-address|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-azure-sql-server-source-address-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:35:03,846] INFO [azure-sql-server-source-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:35:03,848] INFO [azure-sql-server-source-address|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:381)
[2025-04-13 16:35:03,848] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:35:03,848] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:35:03,848] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562103848 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0] Starting SqlServerConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:135)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    connector.class = io.debezium.connector.sqlserver.SqlServerConnector (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.user = kafkadmin@kafkadatabase (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.dbname = database01 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    transforms.unwrap.delete.handling.mode = rewrite (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.names = database01 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.server.name =  (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    schema.history.internal.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.port = 1433 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    topic.prefix = azure_sql_. (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    schema.history.internal.kafka.topic = schema-changes (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    task.class = io.debezium.connector.sqlserver.SqlServerConnectorTask (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.hostname = kafkadatabase.database.windows.net (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    poll.interval.ms = 10000 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    transforms.unwrap.drop.tombstones = true (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    name = azure-sql-server-source-address (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    task.id = 0 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    table.include.list = SalesLT.Address (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:35:03,851] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:35:03,851] INFO [azure-sql-server-source-address|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:35:03,853] INFO [azure-sql-server-source-address|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.SchemaTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1066)
[2025-04-13 16:35:03,854] INFO [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:35:03,854] INFO [azure-sql-server-source-address|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=azure_sql_.-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=azure_sql_.-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-04-13 16:35:03,856] INFO [azure-sql-server-source-address|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=azure_sql_.-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-04-13 16:35:03,856] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_. named = db-history-config-check (io.debezium.util.Threads:271)
[2025-04-13 16:35:03,856] INFO [azure-sql-server-source-address|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:586)
[2025-04-13 16:35:03,856] INFO [azure-sql-server-source-address|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_.-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:35:03,856] INFO [azure-sql-server-source-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:35:03,859] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:35:03,859] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:35:03,859] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562103859 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:35:03,860] INFO [azure-sql-server-source-address|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_.-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = azure_sql_.-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:35:03,860] INFO [azure-sql-server-source-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:35:03,862] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:35:03,862] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:35:03,862] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562103862 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:35:03,862] INFO [azure-sql-server-source-address|task-0] [Producer clientId=azure_sql_.-schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:35:03,864] INFO [azure-sql-server-source-address|task-0] [Consumer clientId=azure_sql_.-schemahistory, groupId=azure_sql_.-schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:35:03,864] INFO [azure-sql-server-source-address|task-0] [Consumer clientId=azure_sql_.-schemahistory, groupId=azure_sql_.-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:35:03,865] INFO [azure-sql-server-source-address|task-0] [Consumer clientId=azure_sql_.-schemahistory, groupId=azure_sql_.-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:35:03,865] INFO [azure-sql-server-source-address|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:35:03,865] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:35:03,865] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:35:03,865] INFO [azure-sql-server-source-address|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:35:03,866] INFO [azure-sql-server-source-address|task-0] App info kafka.consumer for azure_sql_.-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:35:03,866] INFO [azure-sql-server-source-address|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_.-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2025-04-13 16:35:03,867] INFO [azure-sql-server-source-address|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:381)
[2025-04-13 16:35:03,867] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:35:03,867] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:35:03,867] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562103867 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:35:03,904] INFO [azure-sql-server-source-address|task-0] Database schema history topic '(name=schema-changes, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-04-13 16:35:03,907] INFO [azure-sql-server-source-address|task-0] App info kafka.admin.client for azure_sql_.-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:35:03,909] INFO [azure-sql-server-source-address|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:35:03,909] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:35:03,909] INFO [azure-sql-server-source-address|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:35:03,916] INFO [azure-sql-server-source-address|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:378)
[2025-04-13 16:35:03,916] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_. named = SignalProcessor (io.debezium.util.Threads:271)
[2025-04-13 16:35:03,917] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_. named = change-event-source-coordinator (io.debezium.util.Threads:271)
[2025-04-13 16:35:03,917] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_. named = blocking-snapshot (io.debezium.util.Threads:271)
[2025-04-13 16:35:03,917] INFO [azure-sql-server-source-address|task-0] Creating thread debezium-sqlserverconnector-azure_sql_.-change-event-source-coordinator (io.debezium.util.Threads:288)
[2025-04-13 16:35:03,919] INFO [azure-sql-server-source-address|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:131)
[2025-04-13 16:35:03,921] INFO [azure-sql-server-source-address|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:134)
[2025-04-13 16:35:03,921] INFO [azure-sql-server-source-address|task-0] No previous offset has been found (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:78)
[2025-04-13 16:35:03,921] INFO [azure-sql-server-source-address|task-0] According to the connector configuration both schema and data will be snapshotted (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:80)
[2025-04-13 16:35:03,921] INFO [azure-sql-server-source-address|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:121)
[2025-04-13 16:35:03,919] INFO [azure-sql-server-source-address|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-04-13 16:35:03,922] INFO [azure-sql-server-source-address|task-0] Creating thread debezium-sqlserverconnector-azure_sql_.-SignalProcessor (io.debezium.util.Threads:288)
[2025-04-13 16:35:03,922] INFO [azure-sql-server-source-address|task-0] WorkerSourceTask{id=azure-sql-server-source-address-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:279)
[2025-04-13 16:35:03,977] INFO [azure-sql-server-source-address|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:130)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductCategory to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.dbo.BuildVersion to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductModelProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductModel to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.SalesOrderHeader to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.Address to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.Customer to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.SalesOrderDetail to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.dbo.ErrorLog to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.CustomerAddress to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.Product to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:222)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Snapshot step 3 - Locking captured tables [database01.SalesLT.Address] (io.debezium.relational.RelationalSnapshotChangeEventSource:139)
[2025-04-13 16:35:03,981] INFO [azure-sql-server-source-address|task-0] Setting locking timeout to 10 s (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:135)
[2025-04-13 16:35:03,988] INFO [azure-sql-server-source-address|task-0] Executing schema locking (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:140)
[2025-04-13 16:35:03,988] INFO [azure-sql-server-source-address|task-0] Locking table database01.SalesLT.Address (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:147)
[2025-04-13 16:35:03,990] INFO [azure-sql-server-source-address|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:145)
[2025-04-13 16:35:03,993] INFO [azure-sql-server-source-address|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:148)
[2025-04-13 16:35:03,996] INFO [azure-sql-server-source-address|task-0] Reading structure of schema 'database01' (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:209)
[2025-04-13 16:35:04,012] INFO [azure-sql-server-source-address|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:152)
[2025-04-13 16:35:04,012] INFO [azure-sql-server-source-address|task-0] Capturing structure of table database01.SalesLT.Address (io.debezium.relational.RelationalSnapshotChangeEventSource:367)
[2025-04-13 16:35:04,012] WARN [azure-sql-server-source-address|task-0] Mapper for type 'uniqueidentifier' not found. (io.debezium.connector.sqlserver.SqlServerDefaultValueConverter:68)
[2025-04-13 16:35:04,052] WARN [azure-sql-server-source-address|task-0] Cannot parse column default value '(getdate())' to type 'datetime'. Expression evaluation is not supported. (io.debezium.connector.sqlserver.SqlServerDefaultValueConverter:78)
[2025-04-13 16:35:04,064] INFO [azure-sql-server-source-address|task-0] Schema locks released. (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:167)
[2025-04-13 16:35:04,064] INFO [azure-sql-server-source-address|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:164)
[2025-04-13 16:35:04,064] INFO [azure-sql-server-source-address|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:414)
[2025-04-13 16:35:04,064] INFO [azure-sql-server-source-address|task-0] For table 'database01.SalesLT.Address' using select statement: 'SELECT [AddressID], [AddressLine1], [AddressLine2], [City], [StateProvince], [CountryRegion], [PostalCode], [rowguid], [ModifiedDate] FROM [database01].[SalesLT].[Address]' (io.debezium.relational.RelationalSnapshotChangeEventSource:423)
[2025-04-13 16:35:04,065] INFO [azure-sql-server-source-address|task-0] Exporting data from table 'database01.SalesLT.Address' (1 of 1 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:542)
[2025-04-13 16:35:04,098] INFO [azure-sql-server-source-address|task-0] 	 Finished exporting 451 records for table 'database01.SalesLT.Address' (1 of 1 tables); total duration '00:00:00.033' (io.debezium.relational.RelationalSnapshotChangeEventSource:588)
[2025-04-13 16:35:04,106] INFO [azure-sql-server-source-address|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:104)
[2025-04-13 16:35:04,106] INFO [azure-sql-server-source-address|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-04-13 16:35:04,108] INFO [azure-sql-server-source-address|task-0] Removing locking timeout (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:262)
[2025-04-13 16:35:04,113] INFO [azure-sql-server-source-address|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=SqlServerOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.sqlserver.Source:STRUCT}, sourceInfo=SourceInfo [serverName=azure_sql_., changeLsn=NULL, commitLsn=00000115:00000908:0005, eventSerialNo=null, snapshot=FALSE, sourceTime=2025-04-13T16:35:04.061686200Z], snapshotCompleted=true, eventSerialNo=1]] (io.debezium.pipeline.ChangeEventSourceCoordinator:254)
[2025-04-13 16:35:04,113] INFO [azure-sql-server-source-address|task-0] Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:35:04,113] INFO [azure-sql-server-source-address|task-0] Starting streaming (io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator:101)
[2025-04-13 16:35:04,113] INFO [azure-sql-server-source-address|task-0] Last position recorded in offsets is 00000115:00000908:0005(NULL)[1] (io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource:160)
[2025-04-13 16:35:08,923] INFO [azure-sql-server-source-address|task-0] 452 records sent during previous 00:00:05.079, last recorded offset of {server=azure_sql_., database=database01} partition is {commit_lsn=00000115:00000908:0005, snapshot=true, snapshot_completed=true} (io.debezium.connector.common.BaseSourceTask:213)
[2025-04-13 16:35:08,925] WARN [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Error while fetching metadata with correlation id 4 : {azure_sql_.=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 16:35:09,048] WARN [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Error while fetching metadata with correlation id 7 : {azure_sql_..database01.SalesLT.Address=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 16:36:03,850] INFO [azure-sql-server-source-address|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-address-0} Committing offsets for 452 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 16:36:17,852] INFO 87.208.3.218 - - [13/Apr/2025:16:36:17 +0000] "DELETE /connectors/azure-sql-server-source-adress/ HTTP/1.1" 404 81 "-" "PostmanRuntime/7.43.3" 4 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:36:24,538] INFO 87.208.3.218 - - [13/Apr/2025:16:36:24 +0000] "GET /connectors HTTP/1.1" 200 35 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:36:35,707] INFO Successfully processed removal of connector 'azure-sql-server-source-address' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:36:35,707] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source-address config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:36:35,707] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:36:35,707] INFO [azure-sql-server-source-address|worker] Stopping connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:36:35,708] INFO [azure-sql-server-source-address|worker] Scheduled shutdown for WorkerConnector{id=azure-sql-server-source-address} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:36:35,708] INFO [azure-sql-server-source-address|worker] Completed shutdown for WorkerConnector{id=azure-sql-server-source-address} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:36:35,708] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:36:35,708] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:36:35,708] INFO 87.208.3.218 - - [13/Apr/2025:16:36:35 +0000] "DELETE /connectors/azure-sql-server-source-address/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:36:35,709] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=96, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:36:35,711] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=96, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:36:35,711] INFO [azure-sql-server-source-address|task-0] Stopping task azure-sql-server-source-address-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:36:35,711] INFO [azure-sql-server-source-address|worker] Stopping connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:36:35,711] WARN [azure-sql-server-source-address|worker] Ignoring stop request for unowned connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:36:35,711] WARN [azure-sql-server-source-address|worker] Ignoring await stop request for non-present connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:36:39,225] INFO [azure-sql-server-source-address|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:282)
[2025-04-13 16:36:40,711] ERROR [azure-sql-server-source-address|task-0] Graceful stop of task azure-sql-server-source-address-0 failed. (org.apache.kafka.connect.runtime.Worker:1074)
[2025-04-13 16:36:40,711] INFO [Producer clientId=connector-producer-azure-sql-server-source-address-0] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:36:40,712] INFO [Producer clientId=connector-producer-azure-sql-server-source-address-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer:1407)
[2025-04-13 16:36:40,712] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:36:40,715] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:36:40,715] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:36:40,715] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:36:40,715] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:36:40,715] INFO App info kafka.producer for connector-producer-azure-sql-server-source-address-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:36:40,717] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:36:40,717] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 96 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=124, connectorIds=[], taskIds=[], revokedConnectorIds=[azure-sql-server-source-address], revokedTaskIds=[azure-sql-server-source-address-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:36:40,717] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 124 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:36:40,717] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:36:40,717] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:36:40,718] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:36:40,722] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=97, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:36:40,726] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=97, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:36:40,726] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 97 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=124, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:36:40,726] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 124 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:36:40,726] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:36:44,113] INFO [azure-sql-server-source-address|task-0] Finished streaming (io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator:133)
[2025-04-13 16:36:44,113] INFO [azure-sql-server-source-address|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:36:44,114] INFO [azure-sql-server-source-address|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-04-13 16:36:44,114] INFO [azure-sql-server-source-address|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-04-13 16:36:44,115] INFO [azure-sql-server-source-address|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:36:44,116] INFO [azure-sql-server-source-address|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:36:44,116] INFO [azure-sql-server-source-address|task-0] [Producer clientId=azure_sql_.-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] App info kafka.producer for azure_sql_.-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1373)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:36:44,119] INFO [azure-sql-server-source-address|task-0] App info kafka.producer for connector-producer-azure-sql-server-source-address-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:38:25,370] INFO Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:38:25,426] INFO Checking if user has access to CDC table (io.debezium.connector.sqlserver.SqlServerConnector:128)
[2025-04-13 16:38:25,507] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:38:25,507] ERROR Failed testing connection for {connector.class=io.debezium.connector.sqlserver.SqlServerConnector, transforms.unwrap.delete.handling.mode=rewrite, database.user=kafkadmin@kafkadatabase, database.names=database02, tasks.max=1, transforms=unwrap, database.server.name=, schema.history.internal.kafka.bootstrap.servers=localhost:9092, database.port=1433, topic.prefix=azure_sql_, schema.history.internal.kafka.topic=schema-changes, database.hostname=kafkadatabase.database.windows.net, database.password=********, poll.interval.ms=10000, transforms.unwrap.drop.tombstones=true, name=azure-sql-server-source-address, transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState, table.include.list=SalesLT.Address, snapshot.mode=initial} with user '[database.user,null,[],[],true]' (io.debezium.connector.sqlserver.SqlServerConnector:146)
com.microsoft.sqlserver.jdbc.SQLServerException: The database 'database02' is not enabled for Change Data Capture. Ensure that the correct database context is set and retry the operation. To report on the databases enabled for Change Data Capture, query the is_cdc_enabled column in the sys.databases catalog view.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:259)
	at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1695)
	at com.microsoft.sqlserver.jdbc.SQLServerStatement.doExecuteStatement(SQLServerStatement.java:920)
	at com.microsoft.sqlserver.jdbc.SQLServerStatement$StmtExecCmd.doExecute(SQLServerStatement.java:814)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)
	at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)
	at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)
	at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeQuery(SQLServerStatement.java:736)
	at io.debezium.jdbc.JdbcConnection.query(JdbcConnection.java:540)
	at io.debezium.jdbc.JdbcConnection.query(JdbcConnection.java:483)
	at io.debezium.connector.sqlserver.SqlServerConnection.checkIfConnectedUserHasAccessToCDCTable(SqlServerConnection.java:385)
	at io.debezium.connector.sqlserver.SqlServerConnector.validateConnection(SqlServerConnector.java:132)
	at io.debezium.connector.common.RelationalBaseSourceConnector.validate(RelationalBaseSourceConnector.java:42)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:714)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$3(AbstractHerder.java:574)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:38:25,508] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:38:25,511] INFO 87.208.3.218 - - [13/Apr/2025:16:38:25 +0000] "POST /connectors HTTP/1.1" 400 554 "-" "PostmanRuntime/7.43.3" 145 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:38:45,388] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:38:45,390] INFO 87.208.3.218 - - [13/Apr/2025:16:38:45 +0000] "POST /connectors HTTP/1.1" 400 273 "-" "PostmanRuntime/7.43.3" 4 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:39:22,198] INFO Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:39:22,279] INFO Checking if user has access to CDC table (io.debezium.connector.sqlserver.SqlServerConnector:128)
[2025-04-13 16:39:22,293] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:39:22,294] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:39:22,298] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector azure-sql-server-source-address config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:39:22,299] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:39:22,299] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:39:22,299] INFO 87.208.3.218 - - [13/Apr/2025:16:39:22 +0000] "POST /connectors HTTP/1.1" 201 850 "-" "PostmanRuntime/7.43.3" 104 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:39:22,300] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=98, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:39:22,302] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=98, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:39:22,302] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 98 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=125, connectorIds=[azure-sql-server-source-address], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:39:22,302] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 125 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:39:22,302] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:39:22,302] INFO [azure-sql-server-source-address|worker] Creating connector azure-sql-server-source-address of type io.debezium.connector.sqlserver.SqlServerConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:39:22,302] INFO [azure-sql-server-source-address|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:39:22,303] INFO [azure-sql-server-source-address|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:39:22,303] INFO [azure-sql-server-source-address|worker] Instantiated connector azure-sql-server-source-address with version 2.5.1.Final of type class io.debezium.connector.sqlserver.SqlServerConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:39:22,303] INFO [azure-sql-server-source-address|worker] Finished creating connector azure-sql-server-source-address (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:39:22,303] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:39:22,304] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:39:22,305] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:39:22,305] INFO [azure-sql-server-source-address|worker] Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:39:22,349] INFO [azure-sql-server-source-address|worker] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:949)
[2025-04-13 16:39:22,354] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [azure-sql-server-source-address-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:39:22,356] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:39:22,356] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:39:22,357] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=99, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:39:22,360] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=99, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:39:22,360] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 99 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=127, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:39:22,360] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 127 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:39:22,360] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task azure-sql-server-source-address-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:39:22,360] INFO [azure-sql-server-source-address|task-0] Creating task azure-sql-server-source-address-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:39:22,361] INFO [azure-sql-server-source-address|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:39:22,361] INFO [azure-sql-server-source-address|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:39:22,361] INFO [azure-sql-server-source-address|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.sqlserver.SqlServerConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:39:22,362] INFO [azure-sql-server-source-address|task-0] Instantiated task azure-sql-server-source-address-0 with version 2.5.1.Final of type io.debezium.connector.sqlserver.SqlServerConnectorTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:39:22,362] INFO [azure-sql-server-source-address|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:39:22,362] INFO [azure-sql-server-source-address|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task azure-sql-server-source-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:39:22,362] INFO [azure-sql-server-source-address|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:39:22,362] INFO [azure-sql-server-source-address|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task azure-sql-server-source-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:39:22,362] INFO [azure-sql-server-source-address|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task azure-sql-server-source-address-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:39:22,363] WARN [azure-sql-server-source-address|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:101)
[2025-04-13 16:39:22,363] INFO [azure-sql-server-source-address|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:39:22,363] INFO [azure-sql-server-source-address|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2025-04-13 16:39:22,363] INFO [azure-sql-server-source-address|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.sqlserver.SqlServerConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = azure-sql-server-source-address
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:39:22,363] INFO [azure-sql-server-source-address|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-azure-sql-server-source-address-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:39:22,364] INFO [azure-sql-server-source-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:39:22,366] INFO [azure-sql-server-source-address|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:381)
[2025-04-13 16:39:22,366] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:39:22,366] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:39:22,366] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562362366 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:39:22,368] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:39:22,368] INFO [azure-sql-server-source-address|task-0] Starting SqlServerConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:135)
[2025-04-13 16:39:22,368] INFO [azure-sql-server-source-address|task-0]    connector.class = io.debezium.connector.sqlserver.SqlServerConnector (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    database.user = kafkadmin@kafkadatabase (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    transforms.unwrap.delete.handling.mode = rewrite (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    database.names = database01 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    database.server.name =  (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    schema.history.internal.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    database.port = 1433 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    topic.prefix = azure_sql_ (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    schema.history.internal.kafka.topic = schema-changes (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    task.class = io.debezium.connector.sqlserver.SqlServerConnectorTask (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    database.hostname = kafkadatabase.database.windows.net (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    poll.interval.ms = 10000 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    transforms.unwrap.drop.tombstones = true (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    name = azure-sql-server-source-address (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    task.id = 0 (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    table.include.list = SalesLT.Address (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:137)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.sqlserver.SqlServerSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1318)
[2025-04-13 16:39:22,369] INFO [azure-sql-server-source-address|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.SchemaTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1066)
[2025-04-13 16:39:22,371] INFO [azure-sql-server-source-address|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=azure_sql_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=azure_sql_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-04-13 16:39:22,371] INFO [azure-sql-server-source-address|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=azure_sql_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-04-13 16:39:22,371] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = db-history-config-check (io.debezium.util.Threads:271)
[2025-04-13 16:39:22,371] INFO [azure-sql-server-source-address|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:586)
[2025-04-13 16:39:22,371] INFO [azure-sql-server-source-address|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2025-04-13 16:39:22,371] INFO [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:39:22,372] INFO [azure-sql-server-source-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:39:22,375] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:39:22,375] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:39:22,375] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562362375 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:39:22,376] INFO [azure-sql-server-source-address|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = azure_sql_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = azure_sql_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:39:22,376] INFO [azure-sql-server-source-address|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:39:22,378] INFO [azure-sql-server-source-address|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:39:22,378] INFO [azure-sql-server-source-address|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:39:22,378] INFO [azure-sql-server-source-address|task-0] Kafka startTimeMs: 1744562362378 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:39:22,379] INFO [azure-sql-server-source-address|task-0] [Producer clientId=azure_sql_-schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:39:22,381] INFO [azure-sql-server-source-address|task-0] [Consumer clientId=azure_sql_-schemahistory, groupId=azure_sql_-schemahistory] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:39:22,381] INFO [azure-sql-server-source-address|task-0] [Consumer clientId=azure_sql_-schemahistory, groupId=azure_sql_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:39:22,381] INFO [azure-sql-server-source-address|task-0] [Consumer clientId=azure_sql_-schemahistory, groupId=azure_sql_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:39:22,381] INFO [azure-sql-server-source-address|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:39:22,381] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:39:22,381] INFO [azure-sql-server-source-address|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:39:22,382] INFO [azure-sql-server-source-address|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:39:22,383] INFO [azure-sql-server-source-address|task-0] App info kafka.consumer for azure_sql_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:39:22,384] INFO [azure-sql-server-source-address|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:378)
[2025-04-13 16:39:22,385] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = SignalProcessor (io.debezium.util.Threads:271)
[2025-04-13 16:39:22,385] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = change-event-source-coordinator (io.debezium.util.Threads:271)
[2025-04-13 16:39:22,385] INFO [azure-sql-server-source-address|task-0] Requested thread factory for connector SqlServerConnector, id = azure_sql_ named = blocking-snapshot (io.debezium.util.Threads:271)
[2025-04-13 16:39:22,385] INFO [azure-sql-server-source-address|task-0] Creating thread debezium-sqlserverconnector-azure_sql_-change-event-source-coordinator (io.debezium.util.Threads:288)
[2025-04-13 16:39:22,386] INFO [azure-sql-server-source-address|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:131)
[2025-04-13 16:39:22,386] INFO [azure-sql-server-source-address|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:134)
[2025-04-13 16:39:22,386] INFO [azure-sql-server-source-address|task-0] No previous offset has been found (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:78)
[2025-04-13 16:39:22,386] INFO [azure-sql-server-source-address|task-0] According to the connector configuration both schema and data will be snapshotted (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:80)
[2025-04-13 16:39:22,386] INFO [azure-sql-server-source-address|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:121)
[2025-04-13 16:39:22,387] INFO [azure-sql-server-source-address|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-04-13 16:39:22,388] INFO [azure-sql-server-source-address|task-0] Creating thread debezium-sqlserverconnector-azure_sql_-SignalProcessor (io.debezium.util.Threads:288)
[2025-04-13 16:39:22,388] INFO [azure-sql-server-source-address|task-0] WorkerSourceTask{id=azure-sql-server-source-address-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:279)
[2025-04-13 16:39:22,423] INFO [azure-sql-server-source-address|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:130)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductCategory to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.dbo.BuildVersion to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductModelProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductModel to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.SalesOrderHeader to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.Address to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.ProductDescription to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.Customer to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.SalesOrderDetail to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.dbo.ErrorLog to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.CustomerAddress to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,427] INFO [azure-sql-server-source-address|task-0] Adding table database01.SalesLT.Product to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:289)
[2025-04-13 16:39:22,428] INFO [azure-sql-server-source-address|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:222)
[2025-04-13 16:39:22,428] INFO [azure-sql-server-source-address|task-0] Snapshot step 3 - Locking captured tables [database01.SalesLT.Address] (io.debezium.relational.RelationalSnapshotChangeEventSource:139)
[2025-04-13 16:39:22,428] INFO [azure-sql-server-source-address|task-0] Setting locking timeout to 10 s (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:135)
[2025-04-13 16:39:22,435] INFO [azure-sql-server-source-address|task-0] Executing schema locking (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:140)
[2025-04-13 16:39:22,435] INFO [azure-sql-server-source-address|task-0] Locking table database01.SalesLT.Address (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:147)
[2025-04-13 16:39:22,437] INFO [azure-sql-server-source-address|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:145)
[2025-04-13 16:39:22,440] INFO [azure-sql-server-source-address|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:148)
[2025-04-13 16:39:22,445] INFO [azure-sql-server-source-address|task-0] Reading structure of schema 'database01' (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:209)
[2025-04-13 16:39:22,460] INFO [azure-sql-server-source-address|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:152)
[2025-04-13 16:39:22,460] INFO [azure-sql-server-source-address|task-0] Capturing structure of table database01.SalesLT.Address (io.debezium.relational.RelationalSnapshotChangeEventSource:367)
[2025-04-13 16:39:22,460] WARN [azure-sql-server-source-address|task-0] Mapper for type 'uniqueidentifier' not found. (io.debezium.connector.sqlserver.SqlServerDefaultValueConverter:68)
[2025-04-13 16:39:22,507] WARN [azure-sql-server-source-address|task-0] Cannot parse column default value '(getdate())' to type 'datetime'. Expression evaluation is not supported. (io.debezium.connector.sqlserver.SqlServerDefaultValueConverter:78)
[2025-04-13 16:39:22,517] INFO [azure-sql-server-source-address|task-0] Schema locks released. (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:167)
[2025-04-13 16:39:22,517] INFO [azure-sql-server-source-address|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:164)
[2025-04-13 16:39:22,517] INFO [azure-sql-server-source-address|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:414)
[2025-04-13 16:39:22,517] INFO [azure-sql-server-source-address|task-0] For table 'database01.SalesLT.Address' using select statement: 'SELECT [AddressID], [AddressLine1], [AddressLine2], [City], [StateProvince], [CountryRegion], [PostalCode], [rowguid], [ModifiedDate] FROM [database01].[SalesLT].[Address]' (io.debezium.relational.RelationalSnapshotChangeEventSource:423)
[2025-04-13 16:39:22,519] INFO [azure-sql-server-source-address|task-0] Exporting data from table 'database01.SalesLT.Address' (1 of 1 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:542)
[2025-04-13 16:39:22,536] INFO [azure-sql-server-source-address|task-0] 	 Finished exporting 451 records for table 'database01.SalesLT.Address' (1 of 1 tables); total duration '00:00:00.017' (io.debezium.relational.RelationalSnapshotChangeEventSource:588)
[2025-04-13 16:39:22,544] INFO [azure-sql-server-source-address|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:104)
[2025-04-13 16:39:22,544] INFO [azure-sql-server-source-address|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-04-13 16:39:22,545] INFO [azure-sql-server-source-address|task-0] Removing locking timeout (io.debezium.connector.sqlserver.SqlServerSnapshotChangeEventSource:262)
[2025-04-13 16:39:22,549] INFO [azure-sql-server-source-address|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=SqlServerOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.sqlserver.Source:STRUCT}, sourceInfo=SourceInfo [serverName=azure_sql_, changeLsn=NULL, commitLsn=00000115:00000b58:0005, eventSerialNo=null, snapshot=FALSE, sourceTime=2025-04-13T16:39:22.520352200Z], snapshotCompleted=true, eventSerialNo=1]] (io.debezium.pipeline.ChangeEventSourceCoordinator:254)
[2025-04-13 16:39:22,549] INFO [azure-sql-server-source-address|task-0] Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator:425)
[2025-04-13 16:39:22,549] INFO [azure-sql-server-source-address|task-0] Starting streaming (io.debezium.connector.sqlserver.SqlServerChangeEventSourceCoordinator:101)
[2025-04-13 16:39:22,549] INFO [azure-sql-server-source-address|task-0] Last position recorded in offsets is 00000115:00000b58:0005(NULL)[1] (io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource:160)
[2025-04-13 16:39:27,389] INFO [azure-sql-server-source-address|task-0] 452 records sent during previous 00:00:05.027, last recorded offset of {server=azure_sql_, database=database01} partition is {commit_lsn=00000115:00000b58:0005, snapshot=true, snapshot_completed=true} (io.debezium.connector.common.BaseSourceTask:213)
[2025-04-13 16:39:27,391] WARN [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Error while fetching metadata with correlation id 4 : {azure_sql_=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 16:39:27,514] WARN [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Error while fetching metadata with correlation id 7 : {azure_sql_.database01.SalesLT.Address=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1213)
[2025-04-13 16:40:22,368] INFO [azure-sql-server-source-address|task-0|offsets] WorkerSourceTask{id=azure-sql-server-source-address-0} Committing offsets for 452 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:235)
[2025-04-13 16:42:33,231] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Resetting the last seen epoch of partition azure_sql_-0 to 0 since the associated topicId changed from null to UL-EbX3USl6UyAVN33H5ig (org.apache.kafka.clients.Metadata:577)
[2025-04-13 16:42:33,231] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Resetting the last seen epoch of partition azure_sql_.database01.SalesLT.Address-0 to 0 since the associated topicId changed from null to hNqZVVpRS_qwvp68o0WrpA (org.apache.kafka.clients.Metadata:577)
[2025-04-13 16:42:46,697] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:42:46,700] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-adress config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:42:46,700] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:42:46,700] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:42:46,701] INFO 87.208.3.218 - - [13/Apr/2025:16:42:46 +0000] "POST /connectors HTTP/1.1" 201 610 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:42:46,704] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=100, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:42:46,707] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=100, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:42:46,707] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 100 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=128, connectorIds=[sink-to-database02-adress, azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:42:46,707] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 128 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:42:46,708] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:42:46,708] INFO [sink-to-database02-adress|worker] Creating connector sink-to-database02-adress of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:42:46,708] INFO [sink-to-database02-adress|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:42:46,708] INFO [sink-to-database02-adress|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:42:46,708] INFO [sink-to-database02-adress|worker] Instantiated connector sink-to-database02-adress with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:42:46,711] INFO [sink-to-database02-adress|worker] Finished creating connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:42:46,713] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:42:46,713] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:42:46,713] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:42:46,713] INFO [sink-to-database02-adress|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 16:42:46,722] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-adress-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:42:46,723] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:42:46,723] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:42:46,727] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=101, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:42:46,728] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=101, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:42:46,728] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 101 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=130, connectorIds=[sink-to-database02-adress, azure-sql-server-source-address], taskIds=[sink-to-database02-adress-0, azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:42:46,729] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 130 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:42:46,730] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:42:46,730] INFO [sink-to-database02-adress|task-0] Creating task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:42:46,730] INFO [sink-to-database02-adress|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:42:46,730] INFO [sink-to-database02-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:42:46,730] INFO [sink-to-database02-adress|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:42:46,730] INFO [sink-to-database02-adress|task-0] Instantiated task sink-to-database02-adress-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:42:46,731] INFO [sink-to-database02-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:42:46,732] INFO [sink-to-database02-adress|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-adress-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02-adress
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:42:46,732] INFO [sink-to-database02-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:42:46,736] INFO [sink-to-database02-adress|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 16:42:46,737] INFO [sink-to-database02-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:42:46,737] INFO [sink-to-database02-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:42:46,737] INFO [sink-to-database02-adress|task-0] Kafka startTimeMs: 1744562566737 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:42:46,737] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:42:46,737] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Subscribed to topic(s): azure_sql_.database01.SalesLT.Address (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 16:42:46,737] INFO [sink-to-database02-adress|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 16:42:46,738] INFO [sink-to-database02-adress|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;databaseName=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 16:42:46,738] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:42:46,738] INFO [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 16:42:46,738] INFO [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 16:42:46,742] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:42:46,742] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 16:42:46,743] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:42:46,745] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-adress-0-ef3d7703-0bde-403d-8f2e-37b8ece330e8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:42:46,745] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:42:49,746] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-adress-0-ef3d7703-0bde-403d-8f2e-37b8ece330e8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 16:42:49,746] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-adress-0-ef3d7703-0bde-403d-8f2e-37b8ece330e8=Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 16:42:49,748] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-adress-0-ef3d7703-0bde-403d-8f2e-37b8ece330e8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 16:42:49,748] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Notifying assignor about the new Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 16:42:49,748] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Adding newly assigned partitions: azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 16:42:49,749] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Found no committed offset for partition azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 16:42:49,751] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Resetting offset for partition azure_sql_.database01.SalesLT.Address-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 16:42:49,772] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:42:49,852] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:42:49,883] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extractRecordValuePk(FieldsMetadata.java:280)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:105)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:42:49,884] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extractRecordValuePk(FieldsMetadata.java:280)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:105)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
[2025-04-13 16:42:49,884] INFO [sink-to-database02-adress|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 16:42:49,884] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:42:49,884] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Revoke previously assigned partitions azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 16:42:49,884] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Member connector-consumer-sink-to-database02-adress-0-ef3d7703-0bde-403d-8f2e-37b8ece330e8 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 16:42:49,885] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:42:49,885] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:42:50,262] INFO [sink-to-database02-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:42:50,262] INFO [sink-to-database02-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:42:50,262] INFO [sink-to-database02-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:42:50,262] INFO [sink-to-database02-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:42:50,265] INFO [sink-to-database02-adress|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:42:51,993] INFO 87.208.3.218 - - [13/Apr/2025:16:42:51 +0000] "GET /connectors/sink-to-database02-address/status HTTP/1.1" 404 87 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:43:10,107] INFO 87.208.3.218 - - [13/Apr/2025:16:43:10 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 200 2150 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:43:25,195] INFO Successfully processed removal of connector 'sink-to-database02-adress' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:43:25,195] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-adress config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:43:25,196] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:43:25,196] INFO [sink-to-database02-adress|worker] Stopping connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:43:25,196] INFO [sink-to-database02-adress|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02-adress} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:43:25,196] INFO [sink-to-database02-adress|worker] Completed shutdown for WorkerConnector{id=sink-to-database02-adress} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:43:25,197] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:43:25,197] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:43:25,198] INFO 87.208.3.218 - - [13/Apr/2025:16:43:25 +0000] "DELETE /connectors/sink-to-database02-adress/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:43:25,199] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=102, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:43:25,200] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=102, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:43:25,201] INFO [sink-to-database02-adress|worker] Stopping connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:43:25,201] WARN [sink-to-database02-adress|worker] Ignoring stop request for unowned connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:43:25,201] WARN [sink-to-database02-adress|worker] Ignoring await stop request for non-present connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:43:25,201] INFO [sink-to-database02-adress|task-0] Stopping task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:43:25,201] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:43:25,209] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:43:25,209] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 102 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=132, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[sink-to-database02-adress], revokedTaskIds=[sink-to-database02-adress-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:43:25,210] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 132 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:43:25,210] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:43:25,210] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:43:25,210] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:43:25,211] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=103, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:43:25,213] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=103, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:43:25,213] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 103 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=132, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:43:25,213] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 132 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:43:25,213] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:43:49,542] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:43:49,546] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-adress config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:43:49,548] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:43:49,548] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:43:49,549] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=104, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:43:49,549] INFO 87.208.3.218 - - [13/Apr/2025:16:43:49 +0000] "POST /connectors HTTP/1.1" 201 606 "-" "PostmanRuntime/7.43.3" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:43:49,550] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=104, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:43:49,551] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 104 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=133, connectorIds=[sink-to-database02-adress, azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:43:49,551] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 133 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:43:49,551] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:43:49,551] INFO [sink-to-database02-adress|worker] Creating connector sink-to-database02-adress of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:43:49,551] INFO [sink-to-database02-adress|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:43:49,551] INFO [sink-to-database02-adress|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:43:49,552] INFO [sink-to-database02-adress|worker] Instantiated connector sink-to-database02-adress with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:43:49,552] INFO [sink-to-database02-adress|worker] Finished creating connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:43:49,552] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:43:49,553] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:43:49,553] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:43:49,553] INFO [sink-to-database02-adress|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 16:43:49,562] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-adress-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:43:49,562] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:43:49,562] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:43:49,564] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=105, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:43:49,566] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=105, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:43:49,566] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 105 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=135, connectorIds=[sink-to-database02-adress, azure-sql-server-source-address], taskIds=[sink-to-database02-adress-0, azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:43:49,566] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 135 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:43:49,568] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:43:49,568] INFO [sink-to-database02-adress|task-0] Creating task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:43:49,568] INFO [sink-to-database02-adress|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:43:49,568] INFO [sink-to-database02-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:43:49,568] INFO [sink-to-database02-adress|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] Instantiated task sink-to-database02-adress-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:43:49,569] INFO [sink-to-database02-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:43:49,570] INFO [sink-to-database02-adress|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-adress-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02-adress
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:43:49,572] INFO [sink-to-database02-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:43:49,574] INFO [sink-to-database02-adress|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 16:43:49,574] INFO [sink-to-database02-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:43:49,574] INFO [sink-to-database02-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:43:49,574] INFO [sink-to-database02-adress|task-0] Kafka startTimeMs: 1744562629574 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:43:49,576] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Subscribed to topic(s): azure_sql_.database01.SalesLT.Address (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 16:43:49,576] INFO [sink-to-database02-adress|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 16:43:49,576] INFO [sink-to-database02-adress|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 16:43:49,577] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:43:49,577] INFO [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 16:43:49,577] INFO [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 16:43:49,578] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:43:49,581] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:43:49,582] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 16:43:49,583] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:43:49,587] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-adress-0-cab96d6c-dfb3-4c8f-8268-7045b9fd1bfb (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:43:49,587] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:43:52,588] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-adress-0-cab96d6c-dfb3-4c8f-8268-7045b9fd1bfb', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 16:43:52,588] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Finished assignment for group at generation 3: {connector-consumer-sink-to-database02-adress-0-cab96d6c-dfb3-4c8f-8268-7045b9fd1bfb=Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 16:43:52,589] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-sink-to-database02-adress-0-cab96d6c-dfb3-4c8f-8268-7045b9fd1bfb', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 16:43:52,589] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Notifying assignor about the new Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 16:43:52,590] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Adding newly assigned partitions: azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 16:43:52,590] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Found no committed offset for partition azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 16:43:52,591] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Resetting offset for partition azure_sql_.database01.SalesLT.Address-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 16:43:52,610] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:43:52,653] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:43:52,693] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extractRecordValuePk(FieldsMetadata.java:280)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:105)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:43:52,694] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extractRecordValuePk(FieldsMetadata.java:280)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:105)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
[2025-04-13 16:43:52,694] INFO [sink-to-database02-adress|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 16:43:52,694] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:43:52,694] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Revoke previously assigned partitions azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 16:43:52,694] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Member connector-consumer-sink-to-database02-adress-0-cab96d6c-dfb3-4c8f-8268-7045b9fd1bfb sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 16:43:52,694] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:43:52,694] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:43:53,097] INFO [sink-to-database02-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:43:53,098] INFO [sink-to-database02-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:43:53,098] INFO [sink-to-database02-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:43:53,098] INFO [sink-to-database02-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:43:53,100] INFO [sink-to-database02-adress|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:43:54,828] INFO 87.208.3.218 - - [13/Apr/2025:16:43:54 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 200 2150 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:43:57,412] INFO 87.208.3.218 - - [13/Apr/2025:16:43:57 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 200 2150 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:44:08,056] INFO Successfully processed removal of connector 'sink-to-database02-adress' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:44:08,056] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-adress config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:44:08,056] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:44:08,057] INFO [sink-to-database02-adress|worker] Stopping connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:44:08,057] INFO [sink-to-database02-adress|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02-adress} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:44:08,057] INFO [sink-to-database02-adress|worker] Completed shutdown for WorkerConnector{id=sink-to-database02-adress} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:44:08,057] INFO 87.208.3.218 - - [13/Apr/2025:16:44:08 +0000] "DELETE /connectors/sink-to-database02-adress/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:44:08,057] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:44:08,057] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:44:08,059] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=106, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:44:08,061] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=106, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:44:08,061] INFO [sink-to-database02-adress|worker] Stopping connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:44:08,061] INFO [sink-to-database02-adress|task-0] Stopping task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:44:08,061] WARN [sink-to-database02-adress|worker] Ignoring stop request for unowned connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:44:08,061] WARN [sink-to-database02-adress|worker] Ignoring await stop request for non-present connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:44:08,061] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:44:08,062] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:44:08,062] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 106 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=137, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[sink-to-database02-adress], revokedTaskIds=[sink-to-database02-adress-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:44:08,063] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 137 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:44:08,063] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:44:08,063] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:44:08,063] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:44:08,064] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=107, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:44:08,065] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=107, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:44:08,065] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 107 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=137, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:44:08,066] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 137 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:44:08,066] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:44:12,974] INFO 87.208.3.218 - - [13/Apr/2025:16:44:12 +0000] "GET /connectors HTTP/1.1" 200 35 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:44:47,549] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:44:47,553] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:44:47,554] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:44:47,554] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:44:47,554] INFO 87.208.3.218 - - [13/Apr/2025:16:44:47 +0000] "POST /connectors HTTP/1.1" 201 592 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:44:47,556] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=108, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:44:47,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=108, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:44:47,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 108 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=138, connectorIds=[sink-to-database02, azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:44:47,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 138 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:44:47,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:44:47,560] INFO [sink-to-database02|worker] Creating connector sink-to-database02 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:44:47,560] INFO [sink-to-database02|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:44:47,560] INFO [sink-to-database02|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:44:47,560] INFO [sink-to-database02|worker] Instantiated connector sink-to-database02 with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:44:47,560] INFO [sink-to-database02|worker] Finished creating connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:44:47,560] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:44:47,561] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:44:47,561] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:44:47,561] INFO [sink-to-database02|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 16:44:47,566] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:44:47,568] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:44:47,569] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:44:47,570] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=109, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:44:47,571] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=109, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:44:47,571] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 109 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=140, connectorIds=[sink-to-database02, azure-sql-server-source-address], taskIds=[sink-to-database02-0, azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:44:47,571] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 140 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:44:47,571] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:44:47,571] INFO [sink-to-database02|task-0] Creating task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:44:47,572] INFO [sink-to-database02|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:44:47,572] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:44:47,572] INFO [sink-to-database02|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:44:47,572] INFO [sink-to-database02|task-0] Instantiated task sink-to-database02-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:44:47,572] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:44:47,572] INFO [sink-to-database02|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:44:47,573] INFO [sink-to-database02|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:44:47,574] INFO [sink-to-database02|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:44:47,576] INFO [sink-to-database02|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 16:44:47,576] INFO [sink-to-database02|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:44:47,576] INFO [sink-to-database02|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:44:47,576] INFO [sink-to-database02|task-0] Kafka startTimeMs: 1744562687576 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:44:47,576] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:44:47,576] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Subscribed to topic(s): azure_sql_.database01.SalesLT.Address (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 16:44:47,577] INFO [sink-to-database02|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 16:44:47,577] INFO [sink-to-database02|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [ProductID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 16:44:47,577] INFO [sink-to-database02|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:44:47,577] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 16:44:47,577] INFO [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 16:44:47,582] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:44:47,582] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 16:44:47,583] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:44:47,586] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-0-a57ca03c-9084-4a66-a57f-03df83f2bb72 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:44:47,586] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:44:50,588] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-a57ca03c-9084-4a66-a57f-03df83f2bb72', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 16:44:50,589] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Finished assignment for group at generation 1: {connector-consumer-sink-to-database02-0-a57ca03c-9084-4a66-a57f-03df83f2bb72=Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 16:44:50,590] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-sink-to-database02-0-a57ca03c-9084-4a66-a57f-03df83f2bb72', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 16:44:50,590] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Notifying assignor about the new Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 16:44:50,590] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Adding newly assigned partitions: azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 16:44:50,591] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Found no committed offset for partition azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 16:44:50,594] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting offset for partition azure_sql_.database01.SalesLT.Address-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 16:44:50,605] INFO [sink-to-database02|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:44:50,657] INFO [sink-to-database02|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:44:50,694] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extractRecordValuePk(FieldsMetadata.java:280)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:105)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:44:50,695] ERROR [sink-to-database02|task-0] WorkerSinkTask{id=sink-to-database02-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: PK mode for table 'Product' is RECORD_VALUE with configured PK fields [ProductID], but record value schema does not contain field: ProductID
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extractRecordValuePk(FieldsMetadata.java:280)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:105)
	at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
[2025-04-13 16:44:50,695] INFO [sink-to-database02|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 16:44:50,695] INFO [sink-to-database02|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:44:50,695] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Revoke previously assigned partitions azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 16:44:50,696] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Member connector-consumer-sink-to-database02-0-a57ca03c-9084-4a66-a57f-03df83f2bb72 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 16:44:50,696] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:44:50,696] INFO [sink-to-database02|task-0] [Consumer clientId=connector-consumer-sink-to-database02-0, groupId=connect-sink-to-database02] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:44:51,097] INFO [sink-to-database02|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:44:51,097] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:44:51,097] INFO [sink-to-database02|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:44:51,098] INFO [sink-to-database02|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:44:51,099] INFO [sink-to-database02|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:44:51,671] INFO 87.208.3.218 - - [13/Apr/2025:16:44:51 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 404 86 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:44:54,299] INFO 87.208.3.218 - - [13/Apr/2025:16:44:54 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 404 86 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:45:13,628] INFO 87.208.3.218 - - [13/Apr/2025:16:45:13 +0000] "GET /connectors/sink-to-database02/status HTTP/1.1" 200 2143 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:46:42,095] INFO 87.208.3.218 - - [13/Apr/2025:16:46:42 +0000] "DELETE /connectors/sink-to-database02-adress/ HTTP/1.1" 404 76 "-" "PostmanRuntime/7.43.3" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:46:48,215] INFO Successfully processed removal of connector 'sink-to-database02' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1003)
[2025-04-13 16:46:48,216] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02 config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2412)
[2025-04-13 16:46:48,216] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Handling connector-only config update by stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:716)
[2025-04-13 16:46:48,216] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:46:48,216] INFO [sink-to-database02|worker] Scheduled shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:294)
[2025-04-13 16:46:48,216] INFO [sink-to-database02|worker] Completed shutdown for WorkerConnector{id=sink-to-database02} (org.apache.kafka.connect.runtime.WorkerConnector:314)
[2025-04-13 16:46:48,217] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:46:48,217] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:46:48,217] INFO 87.208.3.218 - - [13/Apr/2025:16:46:48 +0000] "DELETE /connectors/sink-to-database02/ HTTP/1.1" 204 0 "-" "PostmanRuntime/7.43.3" 8 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:46:48,219] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=110, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:46:48,221] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=110, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:46:48,222] INFO [sink-to-database02|worker] Stopping connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:451)
[2025-04-13 16:46:48,222] WARN [sink-to-database02|worker] Ignoring stop request for unowned connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:454)
[2025-04-13 16:46:48,222] INFO [sink-to-database02|task-0] Stopping task sink-to-database02-0 (org.apache.kafka.connect.runtime.Worker:1047)
[2025-04-13 16:46:48,222] WARN [sink-to-database02|worker] Ignoring await stop request for non-present connector sink-to-database02 (org.apache.kafka.connect.runtime.Worker:475)
[2025-04-13 16:46:48,222] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2708)
[2025-04-13 16:46:48,224] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2729)
[2025-04-13 16:46:48,224] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 110 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=142, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[sink-to-database02], revokedTaskIds=[sink-to-database02-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:46:48,224] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 142 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:46:48,224] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:46:48,224] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:46:48,224] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:46:48,225] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=111, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:46:48,226] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=111, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:46:48,226] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 111 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=142, connectorIds=[azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:46:48,226] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 142 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:46:48,226] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:46:58,542] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2025-04-13 16:46:58,544] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Connector sink-to-database02-adress config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2425)
[2025-04-13 16:46:58,545] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:46:58,545] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:46:58,546] INFO 87.208.3.218 - - [13/Apr/2025:16:46:58 +0000] "POST /connectors HTTP/1.1" 201 606 "-" "PostmanRuntime/7.43.3" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:46:58,546] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=112, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:46:58,548] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=112, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:46:58,548] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 112 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=143, connectorIds=[sink-to-database02-adress, azure-sql-server-source-address], taskIds=[azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:46:58,548] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 143 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:46:58,548] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connector sink-to-database02-adress (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2077)
[2025-04-13 16:46:58,548] INFO [sink-to-database02-adress|worker] Creating connector sink-to-database02-adress of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:312)
[2025-04-13 16:46:58,549] INFO [sink-to-database02-adress|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:46:58,549] INFO [sink-to-database02-adress|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:46:58,549] INFO [sink-to-database02-adress|worker] Instantiated connector sink-to-database02-adress with version 10.7.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:334)
[2025-04-13 16:46:58,549] INFO [sink-to-database02-adress|worker] Finished creating connector sink-to-database02-adress (org.apache.kafka.connect.runtime.Worker:355)
[2025-04-13 16:46:58,549] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:46:58,549] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:46:58,550] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:46:58,550] INFO [sink-to-database02-adress|worker] Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:51)
[2025-04-13 16:46:58,556] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Tasks [sink-to-database02-adress-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2440)
[2025-04-13 16:46:58,557] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:242)
[2025-04-13 16:46:58,557] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:604)
[2025-04-13 16:46:58,558] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=113, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:665)
[2025-04-13 16:46:58,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=113, memberId='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:842)
[2025-04-13 16:46:58,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Joined group at generation 113 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-10.0.0.4:8083-ea5c92aa-0cbc-4ec7-a84a-56e0f7e3d7e1', leaderUrl='http://10.0.0.4:8083/', offset=145, connectorIds=[sink-to-database02-adress, azure-sql-server-source-address], taskIds=[sink-to-database02-adress-0, azure-sql-server-source-address-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2621)
[2025-04-13 16:46:58,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 145 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1959)
[2025-04-13 16:46:58,559] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Starting task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2002)
[2025-04-13 16:46:58,559] INFO [sink-to-database02-adress|task-0] Creating task sink-to-database02-adress-0 (org.apache.kafka.connect.runtime.Worker:645)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] Instantiated task sink-to-database02-adress-0 with version 10.7.0 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:664)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:677)
[2025-04-13 16:46:58,560] INFO [sink-to-database02-adress|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:683)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task sink-to-database02-adress-0 using the worker config (org.apache.kafka.connect.runtime.Worker:690)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1794)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = sink-to-database02-adress
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [azure_sql_.database01.SalesLT.Address]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-sink-to-database02-adress-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-sink-to-database02-adress
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2025-04-13 16:46:58,561] INFO [sink-to-database02-adress|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:269)
[2025-04-13 16:46:58,563] INFO [sink-to-database02-adress|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:381)
[2025-04-13 16:46:58,563] INFO [sink-to-database02-adress|task-0] Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser:124)
[2025-04-13 16:46:58,563] INFO [sink-to-database02-adress|task-0] Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-04-13 16:46:58,563] INFO [sink-to-database02-adress|task-0] Kafka startTimeMs: 1744562818563 (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-04-13 16:46:58,564] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Subscribed to topic(s): azure_sql_.database01.SalesLT.Address (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer:476)
[2025-04-13 16:46:58,564] INFO [sink-to-database02-adress|task-0] Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:51)
[2025-04-13 16:46:58,564] INFO [sink-to-database02-adress|task-0] JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:sqlserver://kafkadatabase.database.windows.net:1433;database=database02
	connection.user = kafkadmin@kafkadatabase
	db.timezone = UTC
	delete.enabled = false
	dialect.name = SqlServerDatabaseDialect
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = [AddressID]
	pk.mode = record_value
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = SalesLT.Product
	table.types = [TABLE]
	trim.sensitive.log = false
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2025-04-13 16:46:58,564] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:46:58,565] INFO [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:324)
[2025-04-13 16:46:58,565] INFO [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:210)
[2025-04-13 16:46:58,565] INFO [Worker clientId=connect-10.0.0.4:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1988)
[2025-04-13 16:46:58,569] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Cluster ID: PRZJITuEQBeLmWOW7dnIsg (org.apache.kafka.clients.Metadata:364)
[2025-04-13 16:46:58,569] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:936)
[2025-04-13 16:46:58,570] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:46:58,572] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Request joining group due to: need to re-join with the given member-id: connector-consumer-sink-to-database02-adress-0-f7af066c-c733-4651-aba5-1995b7d0f8e2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:46:58,572] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:604)
[2025-04-13 16:47:01,574] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-sink-to-database02-adress-0-f7af066c-c733-4651-aba5-1995b7d0f8e2', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:665)
[2025-04-13 16:47:01,574] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Finished assignment for group at generation 5: {connector-consumer-sink-to-database02-adress-0-f7af066c-c733-4651-aba5-1995b7d0f8e2=Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:663)
[2025-04-13 16:47:01,575] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-sink-to-database02-adress-0-f7af066c-c733-4651-aba5-1995b7d0f8e2', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:842)
[2025-04-13 16:47:01,575] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Notifying assignor about the new Assignment(partitions=[azure_sql_.database01.SalesLT.Address-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:323)
[2025-04-13 16:47:01,576] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Adding newly assigned partitions: azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:57)
[2025-04-13 16:47:01,576] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Found no committed offset for partition azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1506)
[2025-04-13 16:47:01,579] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Resetting offset for partition azure_sql_.database01.SalesLT.Address-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2025-04-13 16:47:01,606] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:01,723] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:01,756] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:01,759] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:01,759] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:01,759] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:01,759] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:04,760] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:04,792] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:04,825] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:04,827] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:04,828] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:04,828] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:04,828] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:07,462] INFO 87.208.3.218 - - [13/Apr/2025:16:47:07 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 200 175 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:47:07,828] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:07,867] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:07,901] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:07,903] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:07,903] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:07,904] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:07,904] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:10,904] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:10,938] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:10,972] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:10,974] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:10,974] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:10,974] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:10,974] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:13,975] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:14,022] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:14,054] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:14,057] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:14,058] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:14,058] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:14,058] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:17,060] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:17,094] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:17,136] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:17,138] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:17,138] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:17,138] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:17,138] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:20,138] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:20,187] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:20,218] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:20,220] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:20,220] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:20,220] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:20,220] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:23,220] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:23,277] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:23,336] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:23,342] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:23,342] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:23,342] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:23,342] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:26,343] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:26,384] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:26,420] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:26,421] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:26,422] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:26,422] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:26,422] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:29,422] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:29,459] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:29,497] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:29,498] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:29,499] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:29,499] INFO [sink-to-database02-adress|task-0] Initializing writer using SQL dialect: SqlServerDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:71)
[2025-04-13 16:47:29,499] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:624)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:114)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:32,500] INFO [sink-to-database02-adress|task-0] Attempting to open connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:80)
[2025-04-13 16:47:32,556] INFO [sink-to-database02-adress|task-0] JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2025-04-13 16:47:32,589] INFO [sink-to-database02-adress|task-0] Checking SqlServer dialect for existence of TABLE "SalesLT"."dbo"."Product" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:589)
[2025-04-13 16:47:32,590] WARN [sink-to-database02-adress|task-0] Write of 451 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:98)
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:300)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:133)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
	at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:26)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection$1ConnectionCommand.doExecute(SQLServerConnection.java:4023)
	at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7627)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3912)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectionCommand(SQLServerConnection.java:4031)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.setCatalog(SQLServerConnection.java:4397)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.switchCatalogs(SQLServerDatabaseMetaData.java:400)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetFromStoredProc(SQLServerDatabaseMetaData.java:349)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getResultSetWithProvidedColumnNames(SQLServerDatabaseMetaData.java:374)
	at com.microsoft.sqlserver.jdbc.SQLServerDatabaseMetaData.getTables(SQLServerDatabaseMetaData.java:561)
	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.tableExists(GenericDatabaseDialect.java:590)
	at io.confluent.connect.jdbc.util.TableDefinitions.get(TableDefinitions.java:61)
	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:64)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
[2025-04-13 16:47:32,591] ERROR [sink-to-database02-adress|task-0] Failing task after exhausting retries; encountered 1 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:119)
[2025-04-13 16:47:32,591] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.
 (org.apache.kafka.connect.runtime.WorkerSinkTask:633)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:32,591] ERROR [sink-to-database02-adress|task-0] WorkerSinkTask{id=sink-to-database02-adress-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:233)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:635)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:344)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:246)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:215)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:128)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:605)
	... 11 more
Caused by: java.sql.SQLException: Exception chain:
com.microsoft.sqlserver.jdbc.SQLServerException: USE statement is not supported to switch between databases. Use a new connection to connect to a different database.

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:159)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	... 12 more
[2025-04-13 16:47:32,591] INFO [sink-to-database02-adress|task-0] Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:170)
[2025-04-13 16:47:32,591] INFO [sink-to-database02-adress|task-0] Closing connection #1 to SqlServer (io.confluent.connect.jdbc.util.CachedConnectionProvider:111)
[2025-04-13 16:47:32,591] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Revoke previously assigned partitions azure_sql_.database01.SalesLT.Address-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:79)
[2025-04-13 16:47:32,592] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] The pause flag in partitions [azure_sql_.database01.SalesLT.Address-0] will be removed due to revocation. (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:83)
[2025-04-13 16:47:32,592] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Member connector-consumer-sink-to-database02-adress-0-f7af066c-c733-4651-aba5-1995b7d0f8e2 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1173)
[2025-04-13 16:47:32,592] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1055)
[2025-04-13 16:47:32,592] INFO [sink-to-database02-adress|task-0] [Consumer clientId=connector-consumer-sink-to-database02-adress-0, groupId=connect-sink-to-database02-adress] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1102)
[2025-04-13 16:47:32,594] INFO [sink-to-database02-adress|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:684)
[2025-04-13 16:47:32,594] INFO [sink-to-database02-adress|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:47:32,594] INFO [sink-to-database02-adress|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:688)
[2025-04-13 16:47:32,594] INFO [sink-to-database02-adress|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:694)
[2025-04-13 16:47:32,595] INFO [sink-to-database02-adress|task-0] App info kafka.consumer for connector-consumer-sink-to-database02-adress-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:88)
[2025-04-13 16:48:13,760] INFO 87.208.3.218 - - [13/Apr/2025:16:48:13 +0000] "GET /connectors/sink-to-database02-adress/status HTTP/1.1" 200 2174 "-" "PostmanRuntime/7.43.3" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-04-13 16:48:22,598] INFO [azure-sql-server-source-address|task-0] [Producer clientId=azure_sql_-schemahistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:48:27,804] INFO [azure-sql-server-source-address|task-0] [Producer clientId=connector-producer-azure-sql-server-source-address-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:53:36,848] INFO [AdminClient clientId=connect-cluster-shared-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:58:22,658] INFO [azure-sql-server-source-address|task-0] [Producer clientId=azure_sql_-schemahistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
[2025-04-13 16:58:37,041] INFO [AdminClient clientId=connect-cluster-shared-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1017)
